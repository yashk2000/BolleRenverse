{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "charRNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPA4VB1ovNF/Cbf/C69lT3q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashk2000/BolleRenverse/blob/master/charRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqWNTw5kgx8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jGUukveoWDb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('percy.txt', 'r') as f:\n",
        "  text = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQ5Rn5Sroeta",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "c0dd78f3-59e2-47de-d662-a92e86b916a6"
      },
      "source": [
        "text[:10000]"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ONE \\n\\nI GO CRUISING WITH \\nEXPLOSIVES \\n\\nThe end of the world started when a pegasus landed on the hood of my car. \\n\\nUp until then, I was having a great afternoon. Technically I wasn’t supposed to be driving \\nbecause I wouldn’t turn sixteen for another week, but my mom and my stepdad, Paul, took my \\nfriend Rachel and me to this private stretch of beach on the South Shore, and Paul let us borrow \\nhis Prius for a short spin. \\n\\nNow, 1 know you\\'re thinking, Wow, that was really irresponsible of him, blah, blah, blah, but Paul \\nknows me pretty well. He\\'s seen me slice up demons and leap out of exploding school buildings, so he \\nprobably figured taking a car a few hundred yards wasn\\'t exactly the most dangerous thing I\\'d ever done. \\n\\nAnyway, Rachel and I were driving along. It was a hot August day. Rachel\\'s red hair was pulled back \\nin a ponytail and she wore a white blouse over her swimsuit. I\\'d never seen her in anything but ratty T- \\nshirts and paint-splattered jeans before, and she looked like a million golden drachmas. \\n\\n\"Oh, pull up right there!\" she told me. \\n\\nWe parked on a ridge overlooking the Atlantic. The sea is always one of my favorite places, but \\ntoday it was especially nice — glittery green and smooth as glass, as though my dad was keeping it calm \\njust for us. \\n\\nMy dad, by the way, is Poseidon. He can do stuff like that. \\n\\n\"So.\" Rachel smiled at me. \"About that invitation.\" \\n\\n\"Oh . . . right.\" I tried to sound excited. I mean, she\\'d asked me to her family\\'s vacation house on St. \\nThomas for three days. I didn\\'t get a lot of offers like that. My family\\'s idea of a fancy vacation was a \\nweekend in a rundown cabin on Long Island with some movie rentals and a couple of frozen pizzas, and \\nhere Rachel\\'s folks were willing to let me tag along to the Caribbean. \\n\\nBesides, I seriously needed a vacation. This su mm er had been the hardest of my life. The idea of \\ntaking a break even for a few days was really tempting. \\n\\n\\n\\nStill, something big was supposed to go down any day now. I was \"on call\" for a mission. Even \\nworse, next week was my birthday. There was this prophecy that said when I turned sixteen, bad things \\nwould happen. \\n\\n\"Percy,\" she said, \"I know the timing is bad. But it\\'s always bad for you, right?\" \\n\\nShe had a point. \\n\\n\"I really want to go,\" 1 promised. \"It\\'s just — \" \\n\\n\"The war.” \\n\\nI nodded. I didn\\'t like talking about it, but Rachel knew. Unlike most mortals, she could see through \\nthe Mist — the magic veil that distorts human vision. She\\'d seen monsters. She\\'d met some of the other \\ndemigods who were fighting the Titans and their allies. She\\'d even been there last su mm er when the \\nchopped-up Lord Kronos rose out of his coffin in a terrible new form, and she\\'d earned my permanent \\nrespect by nailing him in the eye with a blue plastic hairbrush. \\n\\nShe put her hand on my arm. \"Just think about it, okay? We don\\'t leave for a couple of days. My dad . \\n\\n. .\" Her voice faltered. \\n\\n\"Is he giving you a hard time?\" I asked. \\n\\nRachel shook her head in disgust. \"He\\'s trying to be nice to me, which is almost worse. He wants me \\nto go to Clarion Ladies Academy m the fall.\" \\n\\n\"The school where your mom went?\" \\n\\n\"It\\'s a stupid finishing school for society girls, all the way in New Hampshire. Can you see me in \\nfinishing school?\" \\n\\nI admitted the idea sounded pretty dumb. Rachel was into urban art projects and feeding the homeless \\nand going to protest rallies to \"Save the Endangered Yellow-bellied Sapsucker\" and stuff like that. I\\'d \\nnever even seen her wear a dress. It was hard to imagine her learning to be a socialite. \\n\\nShe sighed. \"He thi nks if he does a bunch of nice stuff for me. I\\'ll feel guilty and give in.\" \\n\\n\"Which is why he agreed to let me come with you guys on vacation?\" \\n\\n\"Yes . . . but Percy, you\\'d be doing me a huge favor. It would be so much better if you were with us. \\nBesides, there\\'s something I want to talk — \" She stopped abruptly. \\n\\n\"Something you want to talk about?\" I asked. \"You mean ... so serious we\\'d have to go to St. \\nThomas to talk about it?\" \\n\\nShe pursed her lips. \"Look, just forget it for now. Let\\'s pretend we\\'re a couple of normal people. \\nWe\\'re out for a drive, and we\\'re watching the ocean, and it\\'s nice to be together.\" \\n\\nI could tell something was bothering her, but she put on a brave smile. The sunlight made her hair \\nlook like fire. \\n\\nWe\\'d spent a lot of time together this summer. I hadn\\'t exactly planned it that way, but the more \\nserious things got at camp, the more I found myself needing to call up Rachel and get away, just for some \\n\\n\\n\\nbreathing room. I needed to remind myself that the mortal world was still out there, away from all the \\nmonsters using me as their personal punching bag. \\n\\n\"Okay,\" I said. \"Just a normal afternoon and two normal people.\" \\n\\nShe nodded. \"And so . . . hypothetically, if these two people liked each other, what would it take to \\nget the stupid guy to kiss the girl, huh?\" \\n\\n\"Oh ...\" I felt like one of Apollo\\'s sacred cows — slow, dumb, and bright red. \"Um ...\" \\n\\nI can\\'t pretend I hadn\\'t thought about Rachel. She was so much easier to be around than . . . well, than \\nsome other girls I knew. I didn\\'t have to work hard, or watch what I said, or rack my brain trying to figure \\nout what she was thinking. Rachel didn\\'t hide much. She let you know how she felt. \\n\\nI\\'m not sure what I would have done next — but I was so distracted, I didn\\'t notice the huge black form \\nswooping down from the sky until four hooves landed on the hood of the Prius with a WUMP- WUMP- \\nC PUNCH! \\n\\nHey, boss, a voice said in my head. Nice car! \\n\\nBlackjack the pegasus was an old friend of mine, so I tried not to get too annoyed by the craters he\\'d \\njust put in the hood; but I didn\\'t think my stepdad would be real stoked. \\n\\n\"Blackjack,\" I sighed. \"What are you — \" \\n\\nThen I saw who was riding on his back, and I knew my day was about to get a lot more complicated. \\n\\n\" \\'Sup, Percy.\" \\n\\nCharles Beckendorf, senior counselor for the Hephaestus cabin, would make most monsters cry for \\ntheir mommies. He was huge, with ripped muscles from working on the forges every summer, two years \\nolder than me, and one of the camp\\'s best armorsmiths. He made some seriously ingenious mechanical \\nstuff. A month before, he\\'d rigged a Greek firebomb in the bathroom of a tour bus that was carrying a \\nbunch of monsters across country. The explosion took out a whole legion of Kronos\\'s evil meanies as \\nsoon as the first haipy went flush. \\n\\nBeckendorf was dressed for combat. He wore a bronze breastplate and war helm with black camo \\npants and a sword strapped to his side. His explosives bag was slung over his shoulder. \\n\\n\"Time?\" I asked. \\n\\nHe nodded grimly. \\n\\nA clump formed in my throat. I\\'d known this was coming. We\\'d been planning for it for weeks, but I\\'d \\nhalf hoped it would never happen. \\n\\nRachel looked up at Beckendorf. \"Hi.\" \\n\\n\"Oh, hey. I\\'m Beckendorf. You must be Rachel. Percy\\'s told me . . . uh, I mean he mentioned you.\" \\n\\nRachel raised an eyebrow. \"Really? Good.\" She glanced at Blackjack, who was clopping his hooves \\nagainst the hood of the Prius. \"So I guess you guys have to go save the world now.\" \\n\\n\"Pretty much,\" Beckendorf agreed. \\n\\n\\n\\nI looked at Rachel helplessly. \"Would you tell my mom — \" \\n\\n\"I\\'ll tell her. I\\'m sure she\\'s used to it. And I\\'ll explain to Paul about the hood.\" \\n\\nI nodded my thanks. I figured this might be the last time Paul loaned me his car. \\n\\n\"Good luck.\" Rachel kissed me before I could even react. \"Now, get going, half-blood. Go kill some \\nmonsters for me.\" \\n\\nMy last view of her was sitting in the shotgun seat of the Prius, her arms crossed, watching as \\nBlackjack circled higher and higher, carrying Beckendorf and me into the sky. I wondered what Rachel \\nwanted to talk to me about, and whether I\\'d live long enough to find out. \\n\\n\"So,\" Beckendorf said, \"I\\'m guessing you don\\'t want me to mention that little scene to Annabeth.\" \\n\\n\"Oh, gods,\" I muttered. \"Don\\'t even think about it.\" \\n\\nBeckendorf chuckled, and together we soared out over the Atlantic. \\n\\n\\nIt was almost dark by the time we spotted our target. The Princess Andromeda glowed on the horizon — a \\nhuge cruise ship lit up yellow and white. From a distance, you\\'d think it was just a party ship, not the \\nheadquarters for the Titan lord. Then as you got closer, you might notice the giant figurehead — a dark- \\nhaired maiden in a Greek chiton, wrapped m chains with a look of horror on her face, as if she could \\nsmell the stench of all the monsters she was being forced to carry. \\n\\nSeeing the ship again twisted my gut into knots. I\\'d almost died twice on the Princess Andromeda. \\nNow it was heading straight for New York. \\n\\n\"You know what to do?\" Beckendorf yelled over the wind. \\n\\nI nodded. We\\'d done dry runs at the dockyards in New Jersey, using abandoned ships as our targets. I \\nknew how little time we would have. But I also knew this was our best chance to end Kronos\\'s invasion \\nbefore it ever started. \\n\\n\"Blackjack,\" I said, \"set us down on the lowest stem deck.\" \\n\\nGotcha, boss, he said. Man, I hate seeing that boat. \\n\\nThree years ago, Blackjack had been enslaved on the Princess Andromeda until he\\'d escaped with a \\nlittle help from my friends and me. I figured he\\'d rather have his mane braided like My Little Pony than \\nbe back here again. \\n\\n\"Don\\'t wait for us,\" I told him. \\n\\nBut, boss — \\n\\n\"Trust me,\" I said. \"We\\'ll get out by ourselves.\" \\n\\nBlackjack folded his wings and plummeted toward the boat like a black comet. The wind whistled in \\nmy ears. I saw monsters patrolling the upper decks of the ship — dracaenae snake-women, hellhounds. \\n\\n\\n\\ngiants, and the humanoid seal-demons known as telkhines — but we zipped by so fast, none of them raised \\nthe alarm. We shot down the stem of the boat, and Blackjack spread his wings, lightly coming to a \\nlanding on the lowest deck. I climbed off, feeling queasy. \\n\\nGood luck, boss, Blackjack said. Don\\'t let \\'em turn you into horse meat! \\n\\nWith that, my old friend flew off into the night'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTUtqXCUojNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = tuple(set(text))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0NPfzLQpQKd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "da6dcfe8-7b41-40f4-a245-9864d5f0dde7"
      },
      "source": [
        "encoded[:100]"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([46,  0, 26, 42,  2,  2, 38, 42, 65, 46, 42, 15, 69, 83, 38, 54, 38,\n",
              "        0, 65, 42, 55, 38, 75, 57, 42,  2, 26, 81, 21, 13, 46, 54, 38, 73,\n",
              "       26, 54, 42,  2,  2, 75, 50,  8, 42,  8, 30, 16, 42, 11, 18, 42, 19,\n",
              "       50,  8, 42, 35, 11,  4, 33, 16, 42, 77, 19, 39,  4, 19,  8, 16, 42,\n",
              "       35, 50,  8, 30, 42, 39, 42, 85,  8, 63, 39, 77, 48, 77, 42, 33, 39,\n",
              "       30, 16,  8, 16, 42, 11, 30, 42, 19, 50,  8, 42, 50, 11, 11])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAsLBjrmslvG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "  one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "  \n",
        "  one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "  \n",
        "  one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "  \n",
        "  return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9F_3XOcLsuLu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "f6582312-1e00-4ded-db3e-0eb1073937c7"
      },
      "source": [
        "test_seq = np.array([[3, 5, 1]])\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "\n",
        "print(one_hot)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb-ByFKBwj4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "  batch_size_total = batch_size * seq_length\n",
        "  n_batches = len(arr) // batch_size_total\n",
        "  \n",
        "  arr = arr[:n_batches * batch_size_total]\n",
        "  arr = arr.reshape((batch_size, -1))\n",
        "  \n",
        "  for n in range(0, arr.shape[1], seq_length):\n",
        "    x = arr[:, n:n+seq_length]\n",
        "    y = np.zeros_like(x)\n",
        "    try:\n",
        "        y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "    except IndexError:\n",
        "        y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "    yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZE3o8V_KWnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batches = get_batches(encoded, 8, 50)\n",
        "x, y = next(batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0x64g2FKZc6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "6c03adee-4858-41a8-af4c-8227ebe3fb26"
      },
      "source": [
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x\n",
            " [[46  0 26 42  2  2 38 42 65 46]\n",
            " [ 8 42 78 48 77 19 42 77 39 19]\n",
            " [19 42 19 50  8 42  8 16 63  8]\n",
            " [48 19 74 42  2  2 15 50 28  4]\n",
            " [11 16 74  7 42  2  2 38 42 44]\n",
            " [ 8 77  5 42 19 50 28 77 42 28]\n",
            " [48  4  8  5  7 42 69 39 72 50]\n",
            " [77 42 18  4 11 80  8 30 42 39]]\n",
            "\n",
            "y\n",
            " [[ 0 26 42  2  2 38 42 65 46 42]\n",
            " [42 78 48 77 19 42 77 39 19 42]\n",
            " [42 19 50  8 42  8 16 63  8 42]\n",
            " [19 74 42  2  2 15 50 28  4 11]\n",
            " [16 74  7 42  2  2 38 42 44  8]\n",
            " [77  5 42 19 50 28 77 42 28 77]\n",
            " [ 4  8  5  7 42 69 39 72 50  8]\n",
            " [42 18  4 11 80  8 30 42 39 63]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-9Xy9xRKiRt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "63e5cee6-3c6d-485e-b87a-20cf0f09b0c1"
      },
      "source": [
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU!')\n",
        "else: \n",
        "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbPB1SF7KmGC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharRNN(nn.Module):\n",
        "    \n",
        "  def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
        "    super().__init__()\n",
        "    self.drop_prob = drop_prob\n",
        "    self.n_layers = n_layers\n",
        "    self.n_hidden = n_hidden\n",
        "    self.lr = lr\n",
        "    \n",
        "    self.chars = tokens\n",
        "    self.int2char = dict(enumerate(self.chars))\n",
        "    self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "    \n",
        "    self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
        "    \n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "    \n",
        "    self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "    \n",
        "  \n",
        "  def forward(self, x, hidden):\n",
        "                  \n",
        "    r_output, hidden = self.lstm(x, hidden)\n",
        "    \n",
        "    out = self.dropout(r_output)\n",
        "    \n",
        "    out = out.contiguous().view(-1, self.n_hidden)\n",
        "    \n",
        "    out = self.fc(out)\n",
        "    \n",
        "    return out, hidden\n",
        "  \n",
        "  \n",
        "  def init_hidden(self, batch_size):\n",
        "      \n",
        "    weight = next(self.parameters()).data\n",
        "    \n",
        "    if (train_on_gpu):\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "              weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "    else:\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "    \n",
        "    return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1CAMZ8pLEkq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    \n",
        "  net.train()\n",
        "  \n",
        "  opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  \n",
        "  val_idx = int(len(data)*(1-val_frac))\n",
        "  data, val_data = data[:val_idx], data[val_idx:]\n",
        "  \n",
        "  if(train_on_gpu):\n",
        "    net.cuda()\n",
        "  \n",
        "  counter = 0\n",
        "  n_chars = len(net.chars)\n",
        "  for e in range(epochs):\n",
        "    h = net.init_hidden(batch_size)\n",
        "    \n",
        "    for x, y in get_batches(data, batch_size, seq_length):\n",
        "        counter += 1\n",
        "        \n",
        "        x = one_hot_encode(x, n_chars)\n",
        "        inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "          inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        net.zero_grad()\n",
        "        \n",
        "        output, h = net(inputs, h)\n",
        "        \n",
        "        loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        opt.step()\n",
        "        \n",
        "        if counter % print_every == 0:\n",
        "          val_h = net.init_hidden(batch_size)\n",
        "          val_losses = []\n",
        "          net.eval()\n",
        "          for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            val_h = tuple([each.data for each in val_h])\n",
        "            \n",
        "            inputs, targets = x, y\n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            output, val_h = net(inputs, val_h)\n",
        "            val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "        \n",
        "            val_losses.append(val_loss.item())\n",
        "          \n",
        "          net.train() \n",
        "          \n",
        "          print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                \"Step: {}...\".format(counter),\n",
        "                \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOL12hfBLrHQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "61be2cc1-65f7-419a-969b-66a8525b54f2"
      },
      "source": [
        "n_hidden=512\n",
        "n_layers=2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(86, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=86, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acVaoUIdLtUJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "927072a7-14e9-447f-8a4c-70e051a986eb"
      },
      "source": [
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 50 \n",
        "\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/50... Step: 10... Loss: 1.7014... Val Loss: 1.6350\n",
            "Epoch: 1/50... Step: 20... Loss: 1.6627... Val Loss: 1.6054\n",
            "Epoch: 1/50... Step: 30... Loss: 1.6716... Val Loss: 1.5916\n",
            "Epoch: 2/50... Step: 40... Loss: 1.6676... Val Loss: 1.5885\n",
            "Epoch: 2/50... Step: 50... Loss: 1.6416... Val Loss: 1.5773\n",
            "Epoch: 2/50... Step: 60... Loss: 1.6395... Val Loss: 1.5705\n",
            "Epoch: 2/50... Step: 70... Loss: 1.6821... Val Loss: 1.5654\n",
            "Epoch: 3/50... Step: 80... Loss: 1.6140... Val Loss: 1.5591\n",
            "Epoch: 3/50... Step: 90... Loss: 1.6169... Val Loss: 1.5550\n",
            "Epoch: 3/50... Step: 100... Loss: 1.6273... Val Loss: 1.5498\n",
            "Epoch: 4/50... Step: 110... Loss: 1.6302... Val Loss: 1.5455\n",
            "Epoch: 4/50... Step: 120... Loss: 1.5999... Val Loss: 1.5379\n",
            "Epoch: 4/50... Step: 130... Loss: 1.5998... Val Loss: 1.5329\n",
            "Epoch: 4/50... Step: 140... Loss: 1.6364... Val Loss: 1.5297\n",
            "Epoch: 5/50... Step: 150... Loss: 1.5779... Val Loss: 1.5240\n",
            "Epoch: 5/50... Step: 160... Loss: 1.5724... Val Loss: 1.5222\n",
            "Epoch: 5/50... Step: 170... Loss: 1.5845... Val Loss: 1.5189\n",
            "Epoch: 6/50... Step: 180... Loss: 1.5840... Val Loss: 1.5143\n",
            "Epoch: 6/50... Step: 190... Loss: 1.5655... Val Loss: 1.5083\n",
            "Epoch: 6/50... Step: 200... Loss: 1.5579... Val Loss: 1.5020\n",
            "Epoch: 6/50... Step: 210... Loss: 1.6045... Val Loss: 1.5017\n",
            "Epoch: 7/50... Step: 220... Loss: 1.5478... Val Loss: 1.4948\n",
            "Epoch: 7/50... Step: 230... Loss: 1.5371... Val Loss: 1.4886\n",
            "Epoch: 7/50... Step: 240... Loss: 1.5373... Val Loss: 1.4868\n",
            "Epoch: 8/50... Step: 250... Loss: 1.5578... Val Loss: 1.4870\n",
            "Epoch: 8/50... Step: 260... Loss: 1.5315... Val Loss: 1.4772\n",
            "Epoch: 8/50... Step: 270... Loss: 1.5144... Val Loss: 1.4728\n",
            "Epoch: 8/50... Step: 280... Loss: 1.5625... Val Loss: 1.4696\n",
            "Epoch: 9/50... Step: 290... Loss: 1.5016... Val Loss: 1.4642\n",
            "Epoch: 9/50... Step: 300... Loss: 1.5000... Val Loss: 1.4622\n",
            "Epoch: 9/50... Step: 310... Loss: 1.5008... Val Loss: 1.4626\n",
            "Epoch: 10/50... Step: 320... Loss: 1.5211... Val Loss: 1.4572\n",
            "Epoch: 10/50... Step: 330... Loss: 1.4877... Val Loss: 1.4527\n",
            "Epoch: 10/50... Step: 340... Loss: 1.4800... Val Loss: 1.4530\n",
            "Epoch: 10/50... Step: 350... Loss: 1.5370... Val Loss: 1.4454\n",
            "Epoch: 11/50... Step: 360... Loss: 1.4662... Val Loss: 1.4402\n",
            "Epoch: 11/50... Step: 370... Loss: 1.4697... Val Loss: 1.4425\n",
            "Epoch: 11/50... Step: 380... Loss: 1.4764... Val Loss: 1.4354\n",
            "Epoch: 12/50... Step: 390... Loss: 1.4890... Val Loss: 1.4352\n",
            "Epoch: 12/50... Step: 400... Loss: 1.4619... Val Loss: 1.4329\n",
            "Epoch: 12/50... Step: 410... Loss: 1.4358... Val Loss: 1.4264\n",
            "Epoch: 12/50... Step: 420... Loss: 1.5041... Val Loss: 1.4255\n",
            "Epoch: 13/50... Step: 430... Loss: 1.4431... Val Loss: 1.4228\n",
            "Epoch: 13/50... Step: 440... Loss: 1.4433... Val Loss: 1.4208\n",
            "Epoch: 13/50... Step: 450... Loss: 1.4392... Val Loss: 1.4188\n",
            "Epoch: 14/50... Step: 460... Loss: 1.4622... Val Loss: 1.4167\n",
            "Epoch: 14/50... Step: 470... Loss: 1.4289... Val Loss: 1.4137\n",
            "Epoch: 14/50... Step: 480... Loss: 1.4134... Val Loss: 1.4102\n",
            "Epoch: 14/50... Step: 490... Loss: 1.4726... Val Loss: 1.4097\n",
            "Epoch: 15/50... Step: 500... Loss: 1.4074... Val Loss: 1.4037\n",
            "Epoch: 15/50... Step: 510... Loss: 1.4050... Val Loss: 1.4048\n",
            "Epoch: 15/50... Step: 520... Loss: 1.4064... Val Loss: 1.4003\n",
            "Epoch: 16/50... Step: 530... Loss: 1.4326... Val Loss: 1.3996\n",
            "Epoch: 16/50... Step: 540... Loss: 1.4060... Val Loss: 1.3973\n",
            "Epoch: 16/50... Step: 550... Loss: 1.3868... Val Loss: 1.3928\n",
            "Epoch: 16/50... Step: 560... Loss: 1.4593... Val Loss: 1.3940\n",
            "Epoch: 17/50... Step: 570... Loss: 1.3759... Val Loss: 1.3881\n",
            "Epoch: 17/50... Step: 580... Loss: 1.3934... Val Loss: 1.3915\n",
            "Epoch: 17/50... Step: 590... Loss: 1.3862... Val Loss: 1.3865\n",
            "Epoch: 18/50... Step: 600... Loss: 1.4070... Val Loss: 1.3852\n",
            "Epoch: 18/50... Step: 610... Loss: 1.3839... Val Loss: 1.3836\n",
            "Epoch: 18/50... Step: 620... Loss: 1.3675... Val Loss: 1.3805\n",
            "Epoch: 18/50... Step: 630... Loss: 1.4267... Val Loss: 1.3803\n",
            "Epoch: 19/50... Step: 640... Loss: 1.3603... Val Loss: 1.3779\n",
            "Epoch: 19/50... Step: 650... Loss: 1.3724... Val Loss: 1.3779\n",
            "Epoch: 19/50... Step: 660... Loss: 1.3549... Val Loss: 1.3719\n",
            "Epoch: 20/50... Step: 670... Loss: 1.3804... Val Loss: 1.3803\n",
            "Epoch: 20/50... Step: 680... Loss: 1.3609... Val Loss: 1.3746\n",
            "Epoch: 20/50... Step: 690... Loss: 1.3397... Val Loss: 1.3737\n",
            "Epoch: 20/50... Step: 700... Loss: 1.3926... Val Loss: 1.3729\n",
            "Epoch: 21/50... Step: 710... Loss: 1.3297... Val Loss: 1.3695\n",
            "Epoch: 21/50... Step: 720... Loss: 1.3437... Val Loss: 1.3650\n",
            "Epoch: 21/50... Step: 730... Loss: 1.3428... Val Loss: 1.3618\n",
            "Epoch: 22/50... Step: 740... Loss: 1.3589... Val Loss: 1.3703\n",
            "Epoch: 22/50... Step: 750... Loss: 1.3387... Val Loss: 1.3688\n",
            "Epoch: 22/50... Step: 760... Loss: 1.3188... Val Loss: 1.3681\n",
            "Epoch: 22/50... Step: 770... Loss: 1.3823... Val Loss: 1.3656\n",
            "Epoch: 23/50... Step: 780... Loss: 1.3064... Val Loss: 1.3624\n",
            "Epoch: 23/50... Step: 790... Loss: 1.3256... Val Loss: 1.3578\n",
            "Epoch: 23/50... Step: 800... Loss: 1.3129... Val Loss: 1.3544\n",
            "Epoch: 24/50... Step: 810... Loss: 1.3382... Val Loss: 1.3590\n",
            "Epoch: 24/50... Step: 820... Loss: 1.3174... Val Loss: 1.3590\n",
            "Epoch: 24/50... Step: 830... Loss: 1.2905... Val Loss: 1.3579\n",
            "Epoch: 24/50... Step: 840... Loss: 1.3556... Val Loss: 1.3566\n",
            "Epoch: 25/50... Step: 850... Loss: 1.2886... Val Loss: 1.3525\n",
            "Epoch: 25/50... Step: 860... Loss: 1.3020... Val Loss: 1.3533\n",
            "Epoch: 25/50... Step: 870... Loss: 1.2881... Val Loss: 1.3503\n",
            "Epoch: 26/50... Step: 880... Loss: 1.3111... Val Loss: 1.3539\n",
            "Epoch: 26/50... Step: 890... Loss: 1.2920... Val Loss: 1.3522\n",
            "Epoch: 26/50... Step: 900... Loss: 1.2784... Val Loss: 1.3521\n",
            "Epoch: 26/50... Step: 910... Loss: 1.3350... Val Loss: 1.3486\n",
            "Epoch: 27/50... Step: 920... Loss: 1.2766... Val Loss: 1.3491\n",
            "Epoch: 27/50... Step: 930... Loss: 1.2797... Val Loss: 1.3500\n",
            "Epoch: 27/50... Step: 940... Loss: 1.2858... Val Loss: 1.3476\n",
            "Epoch: 28/50... Step: 950... Loss: 1.2972... Val Loss: 1.3531\n",
            "Epoch: 28/50... Step: 960... Loss: 1.2811... Val Loss: 1.3479\n",
            "Epoch: 28/50... Step: 970... Loss: 1.2665... Val Loss: 1.3454\n",
            "Epoch: 28/50... Step: 980... Loss: 1.3181... Val Loss: 1.3459\n",
            "Epoch: 29/50... Step: 990... Loss: 1.2447... Val Loss: 1.3530\n",
            "Epoch: 29/50... Step: 1000... Loss: 1.2687... Val Loss: 1.3461\n",
            "Epoch: 29/50... Step: 1010... Loss: 1.2564... Val Loss: 1.3444\n",
            "Epoch: 30/50... Step: 1020... Loss: 1.2689... Val Loss: 1.3504\n",
            "Epoch: 30/50... Step: 1030... Loss: 1.2614... Val Loss: 1.3476\n",
            "Epoch: 30/50... Step: 1040... Loss: 1.2387... Val Loss: 1.3440\n",
            "Epoch: 30/50... Step: 1050... Loss: 1.3111... Val Loss: 1.3409\n",
            "Epoch: 31/50... Step: 1060... Loss: 1.2340... Val Loss: 1.3489\n",
            "Epoch: 31/50... Step: 1070... Loss: 1.2490... Val Loss: 1.3439\n",
            "Epoch: 31/50... Step: 1080... Loss: 1.2374... Val Loss: 1.3466\n",
            "Epoch: 32/50... Step: 1090... Loss: 1.2597... Val Loss: 1.3560\n",
            "Epoch: 32/50... Step: 1100... Loss: 1.2517... Val Loss: 1.3435\n",
            "Epoch: 32/50... Step: 1110... Loss: 1.2281... Val Loss: 1.3431\n",
            "Epoch: 32/50... Step: 1120... Loss: 1.2817... Val Loss: 1.3405\n",
            "Epoch: 33/50... Step: 1130... Loss: 1.2216... Val Loss: 1.3470\n",
            "Epoch: 33/50... Step: 1140... Loss: 1.2446... Val Loss: 1.3413\n",
            "Epoch: 33/50... Step: 1150... Loss: 1.2107... Val Loss: 1.3447\n",
            "Epoch: 34/50... Step: 1160... Loss: 1.2474... Val Loss: 1.3422\n",
            "Epoch: 34/50... Step: 1170... Loss: 1.2263... Val Loss: 1.3454\n",
            "Epoch: 34/50... Step: 1180... Loss: 1.2077... Val Loss: 1.3434\n",
            "Epoch: 34/50... Step: 1190... Loss: 1.2731... Val Loss: 1.3406\n",
            "Epoch: 35/50... Step: 1200... Loss: 1.2003... Val Loss: 1.3472\n",
            "Epoch: 35/50... Step: 1210... Loss: 1.2262... Val Loss: 1.3460\n",
            "Epoch: 35/50... Step: 1220... Loss: 1.2001... Val Loss: 1.3487\n",
            "Epoch: 36/50... Step: 1230... Loss: 1.2294... Val Loss: 1.3436\n",
            "Epoch: 36/50... Step: 1240... Loss: 1.2162... Val Loss: 1.3443\n",
            "Epoch: 36/50... Step: 1250... Loss: 1.1844... Val Loss: 1.3483\n",
            "Epoch: 36/50... Step: 1260... Loss: 1.2515... Val Loss: 1.3407\n",
            "Epoch: 37/50... Step: 1270... Loss: 1.1753... Val Loss: 1.3570\n",
            "Epoch: 37/50... Step: 1280... Loss: 1.1939... Val Loss: 1.3459\n",
            "Epoch: 37/50... Step: 1290... Loss: 1.1922... Val Loss: 1.3427\n",
            "Epoch: 38/50... Step: 1300... Loss: 1.2162... Val Loss: 1.3475\n",
            "Epoch: 38/50... Step: 1310... Loss: 1.2059... Val Loss: 1.3480\n",
            "Epoch: 38/50... Step: 1320... Loss: 1.1712... Val Loss: 1.3563\n",
            "Epoch: 38/50... Step: 1330... Loss: 1.2405... Val Loss: 1.3414\n",
            "Epoch: 39/50... Step: 1340... Loss: 1.1703... Val Loss: 1.3588\n",
            "Epoch: 39/50... Step: 1350... Loss: 1.1986... Val Loss: 1.3451\n",
            "Epoch: 39/50... Step: 1360... Loss: 1.1840... Val Loss: 1.3489\n",
            "Epoch: 40/50... Step: 1370... Loss: 1.2045... Val Loss: 1.3525\n",
            "Epoch: 40/50... Step: 1380... Loss: 1.1793... Val Loss: 1.3451\n",
            "Epoch: 40/50... Step: 1390... Loss: 1.1671... Val Loss: 1.3517\n",
            "Epoch: 40/50... Step: 1400... Loss: 1.2225... Val Loss: 1.3443\n",
            "Epoch: 41/50... Step: 1410... Loss: 1.1461... Val Loss: 1.3535\n",
            "Epoch: 41/50... Step: 1420... Loss: 1.1696... Val Loss: 1.3494\n",
            "Epoch: 41/50... Step: 1430... Loss: 1.1634... Val Loss: 1.3545\n",
            "Epoch: 42/50... Step: 1440... Loss: 1.1771... Val Loss: 1.3528\n",
            "Epoch: 42/50... Step: 1450... Loss: 1.1799... Val Loss: 1.3520\n",
            "Epoch: 42/50... Step: 1460... Loss: 1.1306... Val Loss: 1.3535\n",
            "Epoch: 42/50... Step: 1470... Loss: 1.2118... Val Loss: 1.3495\n",
            "Epoch: 43/50... Step: 1480... Loss: 1.1442... Val Loss: 1.3565\n",
            "Epoch: 43/50... Step: 1490... Loss: 1.1615... Val Loss: 1.3505\n",
            "Epoch: 43/50... Step: 1500... Loss: 1.1536... Val Loss: 1.3533\n",
            "Epoch: 44/50... Step: 1510... Loss: 1.1613... Val Loss: 1.3569\n",
            "Epoch: 44/50... Step: 1520... Loss: 1.1551... Val Loss: 1.3586\n",
            "Epoch: 44/50... Step: 1530... Loss: 1.1344... Val Loss: 1.3558\n",
            "Epoch: 44/50... Step: 1540... Loss: 1.1911... Val Loss: 1.3552\n",
            "Epoch: 45/50... Step: 1550... Loss: 1.1190... Val Loss: 1.3619\n",
            "Epoch: 45/50... Step: 1560... Loss: 1.1532... Val Loss: 1.3589\n",
            "Epoch: 45/50... Step: 1570... Loss: 1.1269... Val Loss: 1.3558\n",
            "Epoch: 46/50... Step: 1580... Loss: 1.1529... Val Loss: 1.3633\n",
            "Epoch: 46/50... Step: 1590... Loss: 1.1369... Val Loss: 1.3723\n",
            "Epoch: 46/50... Step: 1600... Loss: 1.1184... Val Loss: 1.3594\n",
            "Epoch: 46/50... Step: 1610... Loss: 1.1847... Val Loss: 1.3582\n",
            "Epoch: 47/50... Step: 1620... Loss: 1.0962... Val Loss: 1.3635\n",
            "Epoch: 47/50... Step: 1630... Loss: 1.1209... Val Loss: 1.3636\n",
            "Epoch: 47/50... Step: 1640... Loss: 1.1171... Val Loss: 1.3641\n",
            "Epoch: 48/50... Step: 1650... Loss: 1.1403... Val Loss: 1.3670\n",
            "Epoch: 48/50... Step: 1660... Loss: 1.1150... Val Loss: 1.3717\n",
            "Epoch: 48/50... Step: 1670... Loss: 1.1100... Val Loss: 1.3605\n",
            "Epoch: 48/50... Step: 1680... Loss: 1.1782... Val Loss: 1.3676\n",
            "Epoch: 49/50... Step: 1690... Loss: 1.0915... Val Loss: 1.3811\n",
            "Epoch: 49/50... Step: 1700... Loss: 1.1162... Val Loss: 1.3711\n",
            "Epoch: 49/50... Step: 1710... Loss: 1.0941... Val Loss: 1.3716\n",
            "Epoch: 50/50... Step: 1720... Loss: 1.1386... Val Loss: 1.3648\n",
            "Epoch: 50/50... Step: 1730... Loss: 1.1136... Val Loss: 1.3772\n",
            "Epoch: 50/50... Step: 1740... Loss: 1.0865... Val Loss: 1.3706\n",
            "Epoch: 50/50... Step: 1750... Loss: 1.1527... Val Loss: 1.3716\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsX0wW76L91B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = 'rnn_20_epoch.net'\n",
        "\n",
        "checkpoint = {'n_hidden': net.n_hidden,\n",
        "              'n_layers': net.n_layers,\n",
        "              'state_dict': net.state_dict(),\n",
        "              'tokens': net.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "  torch.save(checkpoint, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D425PYqMFBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "    \n",
        "    x = np.array([[net.char2int[char]]])\n",
        "    x = one_hot_encode(x, len(net.chars))\n",
        "    inputs = torch.from_numpy(x)\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "      inputs = inputs.cuda()\n",
        "    \n",
        "    h = tuple([each.data for each in h])\n",
        "    out, h = net(inputs, h)\n",
        "\n",
        "    p = F.softmax(out, dim=1).data\n",
        "    if(train_on_gpu):\n",
        "      p = p.cpu() \n",
        "    \n",
        "    if top_k is None:\n",
        "      top_ch = np.arange(len(net.chars))\n",
        "    else:\n",
        "      p, top_ch = p.topk(top_k)\n",
        "      top_ch = top_ch.numpy().squeeze()\n",
        "    \n",
        "    p = p.numpy().squeeze()\n",
        "    char = np.random.choice(top_ch, p=p/p.sum())\n",
        "    \n",
        "    return net.int2char[char], h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0UgOfEMMVGv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(net, size, prime='The', top_k=None):\n",
        "        \n",
        "  if(train_on_gpu):\n",
        "    net.cuda()\n",
        "  else:\n",
        "    net.cpu()\n",
        "  \n",
        "  net.eval()\n",
        "  \n",
        "  chars = [ch for ch in prime]\n",
        "  h = net.init_hidden(1)\n",
        "  for ch in prime:\n",
        "    char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "  chars.append(char)\n",
        "  \n",
        "  for ii in range(size):\n",
        "    char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "    chars.append(char)\n",
        "\n",
        "  return ''.join(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1X8FrvvMeYq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "outputId": "e30d788d-9982-4437-9d87-a53560e1a1ec"
      },
      "source": [
        "print(sample(net, 1000, prime='Percy', top_k=5))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percy ...\" \n",
            "\n",
            "\"You must chore,\" Annabeth asked. \"But you have a picture. You have no cabin what I was seeing thore to spend \n",
            "this wish a lift.\" \n",
            "\n",
            "She was shaken as the enemy armor was almost to bad if he was training the courtyand and the weight of \n",
            "the side of the barrier. I didn't know if what to dunt out of those leforates. \n",
            "\n",
            "Annabeth was still turning and carced up to the campers. \n",
            "\n",
            "\"I've got to bat a little both.\" I thought he was a giant fur from my mom. \n",
            "\n",
            "He looked alout at the silver ground. He slapped his belating as it much, battle war shapping \n",
            "him. He was about in the widdwill and the waint doss armor than a bangera porer of terred and were straight off \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Riptide, but the starusa dreamed on a chard of a shartached. \n",
            "\n",
            "I dree them the staining flow, which wasn't arrow. Her scythe an armor was still buttled a stupid fold of the \n",
            "short. I chasged in the weilding in a highway, we would be back in the stitiand \n",
            "dight, as the battle blade hurt to be saving they left a throne right to \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp42SV5KMezy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cde01597-416f-43ef-ca1e-13a3639b3d29"
      },
      "source": [
        "with open('rnn_20_epoch.net', 'rb') as f:\n",
        "  checkpoint = torch.load(f)\n",
        "    \n",
        "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
        "loaded.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcCts6l2MiMd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 757
        },
        "outputId": "6408c46f-c4e4-4f44-8bf5-5376bfa546c5"
      },
      "source": [
        "print(sample(loaded, 2000, top_k=5, prime=\"It iwas day time \"))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "It iwas day time and all\n",
            "the prince worled him, and went out of the prievas of her\n",
            "freeding. The most\n",
            "change to be and\n",
            "she felt\n",
            "happiness, and the country to to see all at that consequence of the steps. The carriage shouted\n",
            "him, he had asked his humble and couther and soft hand with the same child, that he was saying\n",
            "it..\n",
            "\n",
            "\"The servom means to be makes in a long thing well all so many-came. He's the\n",
            "footman's any anyweren sinceres that that would be the moment of the\n",
            "sees in the water of the district persons of it. Betsine has not\n",
            "come, and I'll see you\n",
            "this if it such simpless of\n",
            "his brother is to be more, that's to be abluctionation of the mother,\" he\n",
            "said to him, \"I\n",
            "don't want to do that he saw it's such that immerse man walked it out; I can't go and see and say,\" he thought of his cloak with steps and with\n",
            "smile.\n",
            "\n",
            "\"The children are as is a long thoughts.\" But there was to talk to him about it as all these whole short, with her\n",
            "shoulders was so\n",
            "such to hand her, and walked to the presence, and went off and the sound.\n",
            "\n",
            "\"Well, as they were\n",
            "a singered in the pickure of anything before you. I have dream of that to the more as a propision of his brother of the people. He's not to talk\n",
            "to her in a carreat.\n",
            "I have never saud, and I can't believe it. As it way of the profit for the party, but when you dade the charm, and I had no love of\n",
            "them. It was a matter of the dishancal\n",
            "time and\n",
            "alloth of it.\"\n",
            "\n",
            "She sat down it, and and all who was\n",
            "standing that itself was all about it.\n",
            "\n",
            "\"Well, have I am to say to your time,\" Ared Vronsky, smiling.\n",
            "\n",
            "\"An ideass of these sight of the strange or this music,\" said Levin, \"I am a sillness of an one as always. How so much all that is an answers in that cape, and so my distonched of the coats, as it's something subjects.\"\n",
            "\n",
            "\"Are I do? What is the centar to the meeting,\" he said. \"What was the same? Yis, I am already?\" he said, with a strange dressimation on the\n",
            "submers that it was not to go, but he was satisfaction, she had been that in the fort man the man \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}