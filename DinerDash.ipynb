{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AISG-Technology-Team/Diner-Dash-Workshop/blob/master/Challenge_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k1uhFSLspLGu"
   },
   "source": [
    "# **Diner Dash Challenge**\n",
    "\n",
    "---\n",
    "\n",
    "## Objective:\n",
    "\n",
    "Using Reinforcement Learning(RL) algorithms and a **maximum training timestep of 10 million**, maximise the average rewards from 100 games/episodes of Diner Dash.\n",
    "\n",
    "## Instructions and Expectations:\n",
    "\n",
    "1. Please use Google Colabs for all computing needs (installing of dependencies, training of model, testing of model, generation of submission, etc). This is to ensure fairness in this competition. You can run multiple notebooks but please take note of the contraints of GPU usage.\n",
    "\n",
    "2. You are required to submit **2 files**: \n",
    "  - A fully ran Google Colab notebook \n",
    "  - A Json file which includes action lists for each seeded environment given \n",
    "  \n",
    "  A function \"Testing of policies and verification of submission\" is provided to save your best algo's action list to a json file. For more information about the submission, please refer to the [workshop repo](https://github.com/AISG-Technology-Team/Diner-Dash-Workshop).\n",
    "\n",
    "3. Please update the \"Details of Submission\" section\n",
    "\n",
    "4. We expect to see that the models are learning during training\n",
    "\n",
    "5. If you have any questions, please discuss within your groups first. Otherwise, please check if the issue is existing on the [workshop repo](https://github.com/AISG-Technology-Team/Diner-Dash-Workshop/issues) or raise one if it is not.\n",
    "\n",
    "## Advice on approach to challenge\n",
    "\n",
    "1. Spend some time to read up about the various RL algos, especially easily implementable baselines\n",
    "\n",
    "2. Split the shortlisted algos among the group\n",
    "\n",
    "3. You can choose to train for fewer timesteps and later on further train the model\n",
    "\n",
    "4. Take note of the training duration. Time is tight!\n",
    "\n",
    "5. If necessary, tune the hyperparameters to ensure learning\n",
    "\n",
    "6. Have fun!\n",
    "\n",
    "## Important Resources:\n",
    "\n",
    "1. [Diner Dash repo](https://github.com/AdaCompNUS/diner-dash-simulator)\n",
    "\n",
    "2. [Workshop repo](https://github.com/AISG-Technology-Team/Diner-Dash-Workshop)\n",
    "\n",
    "3. [Stable Baselines](https://github.com/hill-a/stable-baselines)\n",
    "\n",
    "## Things to note:\n",
    "\n",
    "1. Please change the runtime to a GPU when using a GPU. In the above tabs, click Runtime > Change runtime type > GPU in the Hardware accelerator dropdown\n",
    "\n",
    "2. If an \"Error: A module (diner_dash) was specified for the environment but was not found, make sure the package is installed with `pip install` before calling `gym.make()`\" error is raised, please restart the runtime and rerun the installation of the diner dash simulator.\n",
    "\n",
    "2. Please ensure a strong internet connection throughout this challenge to avoid disconnecting from the collab GPUs\n",
    "\n",
    "3. Do not idle your computer as collab automatically disconnects GPUs if the idle time is too long\n",
    "\n",
    "4. GPUs run on CUDA 10.1\n",
    "\n",
    "For other FAQs, refer to this [link](https://research.google.com/colaboratory/faq.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hLMQrYZNhc12"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hLhRQoYbhGvh"
   },
   "source": [
    "# Details of Submission [Please Edit]\n",
    "\n",
    "### Team Name / ID:\n",
    "AISG Engineers / Team 32\n",
    "\n",
    "### Names of Group Members:\n",
    "Ban Kar Weng, Yash Khare, Wee Yeong Loo\n",
    "\n",
    "### Names of Algorithms tested:\n",
    "Random Agent, PPO, PPO2, A2C, ACKTR, ACER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h0Sr4eQLrZ3f"
   },
   "source": [
    "# Information on Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xCjo39oa4Wo_"
   },
   "source": [
    "## Python Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GwHyupUqn9I0",
    "outputId": "642a54c9-9da2-45c8-eb1d-be5e2120add8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.9\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rpmpiz2E4ZIi"
   },
   "source": [
    "## Cuda Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "P_YqJEiT4LhX",
    "outputId": "1f739815-40ec-4f43-f950-155bb6217b42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2019 NVIDIA Corporation\n",
      "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
      "Cuda compilation tools, release 10.1, V10.1.243\n"
     ]
    }
   ],
   "source": [
    "!nvcc -V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CGTjEEIFFH5u"
   },
   "source": [
    "# Mounting Google Drive\n",
    "\n",
    "To store trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "h1zPeIVOE2c6",
    "outputId": "e723823d-546f-4c1d-af03-f6949d40ce91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fb_lw5zhJ6Av"
   },
   "source": [
    "## Create Project Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "r4uwe83-Fo0E",
    "outputId": "3c743a80-1353-4c04-a68d-59be5f69dcbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project DinerDashChallenge already exist!\n",
      "The current working directory is /content/drive/My Drive/DinerDashChallenge\n"
     ]
    }
   ],
   "source": [
    "from os import path, chdir, getcwd, mkdir\n",
    "\n",
    "# Choose a project name\n",
    "projectName = \"DinerDashChallenge\"\n",
    "\n",
    "# Project directory is in My Drive\n",
    "projectDirectory = \"/content/drive/My Drive/\" + projectName\n",
    "\n",
    "# Checks if cwd is in content folder\n",
    "if getcwd() == \"/content\":\n",
    "  # Makes project directory if it does not exist\n",
    "  if not path.isdir(projectDirectory):\n",
    "    mkdir(projectDirectory)\n",
    "    print(f\"Project {projectName} has been created!\")\n",
    "  else:\n",
    "    print(f\"Project {projectName} already exist!\")\n",
    "  # Changes to project directory\n",
    "  chdir(projectDirectory)\n",
    "\n",
    "print(f\"The current working directory is {getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rhJmt90nhfuz"
   },
   "source": [
    "# Installing Dependencies\n",
    "\n",
    "Downloading relevant project dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4iYXlsBohnCo"
   },
   "source": [
    "## Dependencies for [diner dash simulator](https://github.com/AdaCompNUS/diner-dash-simulator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "h24WNb92iLo5",
    "outputId": "44a394c2-1789-410c-dc0d-daf7454d5913"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diner Dash repo is already available at /content/drive/My Drive/DinerDashChallenge/diner-dash-simulator\n"
     ]
    }
   ],
   "source": [
    "from os import path, getcwd\n",
    "\n",
    "repoName = \"diner-dash-simulator\"\n",
    "\n",
    "# Clones repo if it does not exist\n",
    "if not path.isdir(repoName):\n",
    "  !git clone https://github.com/AdaCompNUS/diner-dash-simulator.git\n",
    "  print(f\"Diner Dash repo has been cloned to {getcwd()}\")\n",
    "else:\n",
    "  print(f\"Diner Dash repo is already available at {path.join(getcwd(), repoName)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "ArJDnBT3eq_g",
    "outputId": "355634e2-8b9c-4069-f830-9f394225fc24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///content/drive/My%20Drive/DinerDashChallenge/diner-dash-simulator/DinerDashEnv\n",
      "Requirement already satisfied: gym>=0.2.3 in /usr/local/lib/python3.6/dist-packages (from diner-dash==0.0.1) (0.17.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from diner-dash==0.0.1) (1.18.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from diner-dash==0.0.1) (2.23.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.2.3->diner-dash==0.0.1) (1.5.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym>=0.2.3->diner-dash==0.0.1) (1.4.1)\n",
      "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.2.3->diner-dash==0.0.1) (1.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->diner-dash==0.0.1) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->diner-dash==0.0.1) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->diner-dash==0.0.1) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->diner-dash==0.0.1) (3.0.4)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.2.3->diner-dash==0.0.1) (0.16.0)\n",
      "Installing collected packages: diner-dash\n",
      "  Found existing installation: diner-dash 0.0.1\n",
      "    Can't uninstall 'diner-dash'. No files were found to uninstall.\n",
      "  Running setup.py develop for diner-dash\n",
      "Successfully installed diner-dash\n"
     ]
    }
   ],
   "source": [
    "!pip install -e diner-dash-simulator/DinerDashEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yFG_CNITTHDk",
    "outputId": "8b88b4b3-11c3-42a9-8878-a83ed1f526c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installation of diner dash simulator is successful!\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# Test make environment\n",
    "def testEnv():\n",
    "  env = gym.make('diner_dash:DinerDash-v0').unwrapped\n",
    "  env.flash_sim = False\n",
    "  env.close()\n",
    "  return True\n",
    "\n",
    "if testEnv():\n",
    "  print(\"Installation of diner dash simulator is successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sI9AG8cflhV2"
   },
   "source": [
    "## Dependencies for Policy [Please Edit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "tZQV-Quxmd_H",
    "outputId": "afa105f1-5e44-49b6-9d13-399bd7ce1243"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.5.1+cu101 in /usr/local/lib/python3.6/dist-packages (1.5.1+cu101)\n",
      "Requirement already satisfied: torchvision==0.6.1+cu101 in /usr/local/lib/python3.6/dist-packages (0.6.1+cu101)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.18.5)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu101) (7.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "t3vS8KYfpIan",
    "outputId": "5dd3d617-349c-4529-9de3-fc881bd88f99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n",
      "Requirement already satisfied: stable-baselines[mpi]==2.10.0 in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (3.2.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (0.16.0)\n",
      "Requirement already satisfied: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (0.17.2)\n",
      "Requirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (1.3.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (1.4.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (1.18.5)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (4.1.2.30)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (1.0.5)\n",
      "Requirement already satisfied: mpi4py; extra == \"mpi\" in /tensorflow-1.15.2/python3.6 (from stable-baselines[mpi]==2.10.0) (3.0.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (2.8.1)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.5.0)\n",
      "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.2.6)\n",
      "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (7.0.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines[mpi]==2.10.0) (2018.9)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->stable-baselines[mpi]==2.10.0) (1.15.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Stable Baselines only supports tensorflow 1.x for now\n",
    "%tensorflow_version 1.x\n",
    "!pip install stable-baselines[mpi]==2.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rzdKjy68IIBL"
   },
   "source": [
    "# Check GPU usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jqvFkTbo69ig",
    "outputId": "bbdfe7c7-3b0a-42f3-a185-df9373816e9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU runtime is in use!\n"
     ]
    }
   ],
   "source": [
    "# Check if runtime uses GPU\n",
    "# Ignore error if you do not wish to use a GPU\n",
    "from tensorflow.test import gpu_device_name\n",
    "\n",
    "device_name = gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  print(\n",
    "      '\\n\\nThis error most likely means that this notebook is not '\n",
    "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
    "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
    "  raise SystemError('GPU device not found')\n",
    "else:\n",
    "  print(\"GPU runtime is in use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jhLb7POHP-Ic"
   },
   "source": [
    "# Helper Functions\n",
    "\n",
    "For easier debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WMDW4nVXQA4Z"
   },
   "outputs": [],
   "source": [
    "def getAction(actionID):\n",
    "    actionIDtoName = {\n",
    "        0 : \"None\",\n",
    "        1 : \"Move to Table 1\",\n",
    "        2 : \"Move to Table 2\",\n",
    "        3 : \"Move to Table 3\",\n",
    "        4 : \"Move to Table 4\",\n",
    "        5 : \"Move to Table 5\",\n",
    "        6 : \"Move to Table 6\",\n",
    "        7 : \"Move to Counter\",\n",
    "        8 : \"Pick Food for Table 1\",\n",
    "        9 : \"Pick Food for Table 2\",\n",
    "        10 : \"Pick Food for Table 3\",\n",
    "        11 : \"Pick Food for Table 4\",\n",
    "        12 : \"Pick Food for Table 5\",\n",
    "        13 : \"Pick Food for Table 6\",\n",
    "        14 : \"Move to Food Collection\",\n",
    "        15 : \"Pick Table 1 for Group 1\",\n",
    "        16 : \"Pick Table 2 for Group 1\",\n",
    "        17 : \"Pick Table 3 for Group 1\",\n",
    "        18 : \"Pick Table 4 for Group 1\",\n",
    "        19 : \"Pick Table 5 for Group 1\",\n",
    "        20 : \"Pick Table 6 for Group 1\",\n",
    "        21 : \"Pick Table 1 for Group 2\",\n",
    "        22 : \"Pick Table 2 for Group 2\",\n",
    "        23 : \"Pick Table 3 for Group 2\",\n",
    "        24 : \"Pick Table 4 for Group 2\",\n",
    "        25 : \"Pick Table 5 for Group 2\",\n",
    "        26 : \"Pick Table 6 for Group 2\",\n",
    "        27 : \"Pick Table 1 for Group 3\",\n",
    "        28 : \"Pick Table 2 for Group 3\",\n",
    "        29 : \"Pick Table 3 for Group 3\",\n",
    "        30 : \"Pick Table 4 for Group 3\",\n",
    "        31 : \"Pick Table 5 for Group 3\",\n",
    "        32 : \"Pick Table 6 for Group 3\",\n",
    "        33 : \"Pick Table 1 for Group 4\",\n",
    "        34 : \"Pick Table 2 for Group 4\",\n",
    "        35 : \"Pick Table 3 for Group 4\",\n",
    "        36 : \"Pick Table 4 for Group 4\",\n",
    "        37 : \"Pick Table 5 for Group 4\",\n",
    "        38 : \"Pick Table 6 for Group 4\",\n",
    "        39 : \"Pick Table 1 for Group 5\",\n",
    "        40 : \"Pick Table 2 for Group 5\",\n",
    "        41 : \"Pick Table 3 for Group 5\",\n",
    "        42 : \"Pick Table 4 for Group 5\",\n",
    "        43 : \"Pick Table 5 for Group 5\",\n",
    "        44 : \"Pick Table 6 for Group 5\",\n",
    "        45 : \"Pick Table 1 for Group 6\",\n",
    "        46 : \"Pick Table 2 for Group 6\",\n",
    "        47 : \"Pick Table 3 for Group 6\",\n",
    "        48 : \"Pick Table 4 for Group 6\",\n",
    "        49 : \"Pick Table 5 for Group 6\",\n",
    "        50 : \"Pick Table 6 for Group 6\",\n",
    "        51 : \"Pick Table 1 for Group 7\",\n",
    "        52 : \"Pick Table 2 for Group 7\",\n",
    "        53 : \"Pick Table 3 for Group 7\",\n",
    "        54 : \"Pick Table 4 for Group 7\",\n",
    "        55 : \"Pick Table 5 for Group 7\",\n",
    "        56 : \"Pick Table 6 for Group 7\",\n",
    "    }\n",
    "    return actionIDtoName[actionID]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sIfwpn5wnTwk"
   },
   "source": [
    "# Policies [Please Edit]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IbkVfqVlKED_"
   },
   "source": [
    "## Initialise Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IN5uGdnlqqmi"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Initialises first env\n",
    "def initEnv(seed=None):\n",
    "  env = gym.make('diner_dash:DinerDash-v0').unwrapped\n",
    "  env.flash_sim = False\n",
    "  \n",
    "  if seed != None:\n",
    "    # sets random seed\n",
    "    env.seed(seed)\n",
    "\n",
    "  obs = env.reset()\n",
    "\n",
    "  return env, obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "47LrGgLLHgzy"
   },
   "source": [
    "## Self Implemented/Adapted Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ul2Fi9PaPgz7"
   },
   "source": [
    "### Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "StTisAOnPvoS"
   },
   "outputs": [],
   "source": [
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W4cYi3VuPjwg"
   },
   "outputs": [],
   "source": [
    "# Randomly select an action from the action space\n",
    "def testRA(seed):\n",
    "\n",
    "  # init env\n",
    "  env, _ = initEnv(seed=seed)\n",
    "\n",
    "  # init variables\n",
    "  done = False\n",
    "  sumReward = 0\n",
    "  actionList = []\n",
    "\n",
    "  while not done:\n",
    "      action = randint(0, 56)\n",
    "      actionList.append(action)\n",
    "      state, reward, done, _ = env.step(action)\n",
    "      sumReward += reward\n",
    "\n",
    "  return sumReward, actionList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ogJEf9DnYzs"
   },
   "source": [
    "### [PPO](https://github.com/nikhilbarhate99/PPO-PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EaLiVAuEnP4M",
    "outputId": "421ca700-55f6-4da4-867a-653517237d8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pbJ-Mr5ZoCmT"
   },
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, n_latent_var):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        # actor\n",
    "        self.action_layer = nn.Sequential(\n",
    "                nn.Linear(state_dim, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, action_dim),\n",
    "                nn.Softmax(dim=-1)\n",
    "                )\n",
    "        \n",
    "        # critic\n",
    "        self.value_layer = nn.Sequential(\n",
    "                nn.Linear(state_dim, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, 1)\n",
    "                )\n",
    "        \n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def act(self, state, memory):\n",
    "        state = torch.from_numpy(state).float().to(device) \n",
    "        action_probs = self.action_layer(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        memory.states.append(state)\n",
    "        memory.actions.append(action)\n",
    "        memory.logprobs.append(dist.log_prob(action))\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def evaluate(self, state, action):\n",
    "        action_probs = self.action_layer(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        \n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        \n",
    "        state_value = self.value_layer(state)\n",
    "        \n",
    "        return action_logprobs, torch.squeeze(state_value), dist_entropy\n",
    "        \n",
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip):\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        \n",
    "        self.policy = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "    \n",
    "    def update(self, memory):   \n",
    "        # Monte Carlo estimate of state rewards:\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "        \n",
    "        # Normalizing the rewards:\n",
    "        rewards = torch.tensor(rewards).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "        \n",
    "        # convert list to tensor\n",
    "        old_states = torch.stack(memory.states).to(device).detach()\n",
    "        old_actions = torch.stack(memory.actions).to(device).detach()\n",
    "        old_logprobs = torch.stack(memory.logprobs).to(device).detach()\n",
    "        \n",
    "        # Optimize policy for K epochs:\n",
    "        for _ in range(self.K_epochs):\n",
    "            # Evaluating old actions and values :\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "            \n",
    "            # Finding the ratio (pi_theta / pi_theta__old):\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "                \n",
    "            # Finding Surrogate Loss:\n",
    "            advantages = rewards - state_values.detach()\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
    "            \n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        # Copy new weights into old policy:\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hb-_zyFloDUt"
   },
   "outputs": [],
   "source": [
    "def trainPPO():\n",
    "    ############## Hyperparameters ##############\n",
    "    # creating environment\n",
    "    env, _ = initEnv()\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    log_directory = \"./logs\"     # log directory\n",
    "    log_interval = 500          # print avg reward in the interval\n",
    "    save_interval = int(5e5)    # checkpoints to save model\n",
    "    max_timesteps = int(1e7)    # max training timesteps\n",
    "    n_latent_var = 64           # number of variables in hidden layer\n",
    "    update_timestep = 2000      # update policy every n timesteps\n",
    "    lr = 0.003\n",
    "    betas = (0.9, 0.999)\n",
    "    gamma = 0.99                # discount factor\n",
    "    K_epochs = 4                # update policy for K epochs\n",
    "    eps_clip = 0.2              # clip parameter for PPO\n",
    "    random_seed = None\n",
    "    #############################################\n",
    "    \n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "\n",
    "    if random_seed:\n",
    "        torch.manual_seed(random_seed)\n",
    "        env.seed(random_seed)\n",
    "    \n",
    "    memory = Memory()\n",
    "    ppo = PPO(state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip)\n",
    "    print(lr,betas)\n",
    "    \n",
    "    # logging variables\n",
    "    running_reward = 0\n",
    "    avg_length = 0\n",
    "    timestep = 0 # train timesteps\n",
    "    t = 0 # timestep within each episode\n",
    "    e = 0 # num of episodes\n",
    "\n",
    "    done = False\n",
    "    \n",
    "    # training loop\n",
    "    while timestep <= max_timesteps:\n",
    "        state = env.reset()\n",
    "        e += 1 # episode number\n",
    "        while not done:\n",
    "            timestep += 1\n",
    "            t += 1 # timestep within each episode\n",
    "\n",
    "            if timestep == max_timesteps:\n",
    "                torch.save(ppo.policy.state_dict(), f'./PPO_diner-dash_{timestep:.0e}.pth')\n",
    "                print(f\"--- Time take to train model = {(time.time() - start_time)//60} minutes ---\")\n",
    "                return\n",
    "            \n",
    "            # Running policy_old:\n",
    "            action = ppo.policy_old.act(state, memory)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Saving reward and is_terminal:\n",
    "            memory.rewards.append(reward)\n",
    "            memory.is_terminals.append(done)\n",
    "            \n",
    "            # update if its time\n",
    "            if timestep % update_timestep == 0:\n",
    "                ppo.update(memory)\n",
    "                memory.clear_memory()\n",
    "\n",
    "            # save model at checkpoints\n",
    "            if timestep % save_interval == 0:\n",
    "                if not path.isdir(log_directory):\n",
    "                    mkdir(log_directory)\n",
    "                torch.save(ppo.policy.state_dict(), f'{log_directory}/PPO_diner-dash_{timestep:.0e}.pth')\n",
    "            \n",
    "            running_reward += reward\n",
    "                \n",
    "        avg_length += t\n",
    "\n",
    "        # reset timestep t and done since episode ended\n",
    "        t = 0\n",
    "        done = False\n",
    "            \n",
    "        # logging\n",
    "        if e % log_interval == 0:\n",
    "            avg_length = int(avg_length/log_interval)\n",
    "            running_reward = int((running_reward/log_interval))\n",
    "            \n",
    "            print('Episode {} \\t avg length: {} \\t reward: {}'.format(e, avg_length, running_reward))\n",
    "            running_reward = 0\n",
    "            avg_length = 0\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4c06KtkyZLHC",
    "outputId": "8e09737c-ac54-430e-e6d3-5c299543781d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003 (0.9, 0.999)\n",
      "Episode 500 \t avg length: 137 \t reward: -1131\n",
      "Episode 1000 \t avg length: 137 \t reward: -968\n",
      "Episode 1500 \t avg length: 138 \t reward: -972\n",
      "Episode 2000 \t avg length: 137 \t reward: -675\n",
      "Episode 2500 \t avg length: 136 \t reward: -490\n",
      "Episode 3000 \t avg length: 136 \t reward: -245\n",
      "Episode 3500 \t avg length: 135 \t reward: -122\n",
      "Episode 4000 \t avg length: 136 \t reward: -10\n",
      "Episode 4500 \t avg length: 137 \t reward: 42\n",
      "Episode 5000 \t avg length: 138 \t reward: 4\n",
      "Episode 5500 \t avg length: 136 \t reward: 105\n",
      "Episode 6000 \t avg length: 136 \t reward: 166\n",
      "Episode 6500 \t avg length: 138 \t reward: 201\n",
      "Episode 7000 \t avg length: 138 \t reward: 208\n",
      "Episode 7500 \t avg length: 138 \t reward: 199\n",
      "Episode 8000 \t avg length: 137 \t reward: 228\n",
      "Episode 8500 \t avg length: 138 \t reward: 261\n",
      "Episode 9000 \t avg length: 139 \t reward: 277\n",
      "Episode 9500 \t avg length: 138 \t reward: 326\n",
      "Episode 10000 \t avg length: 138 \t reward: 253\n",
      "Episode 10500 \t avg length: 139 \t reward: 219\n",
      "Episode 11000 \t avg length: 141 \t reward: 262\n",
      "Episode 11500 \t avg length: 140 \t reward: 212\n",
      "Episode 12000 \t avg length: 140 \t reward: 212\n",
      "Episode 12500 \t avg length: 140 \t reward: 199\n",
      "Episode 13000 \t avg length: 141 \t reward: 201\n",
      "Episode 13500 \t avg length: 142 \t reward: 219\n",
      "Episode 14000 \t avg length: 141 \t reward: 297\n",
      "Episode 14500 \t avg length: 143 \t reward: 313\n",
      "Episode 15000 \t avg length: 143 \t reward: 402\n",
      "Episode 15500 \t avg length: 144 \t reward: 386\n",
      "Episode 16000 \t avg length: 144 \t reward: 449\n",
      "Episode 16500 \t avg length: 145 \t reward: 424\n",
      "Episode 17000 \t avg length: 144 \t reward: 453\n",
      "Episode 17500 \t avg length: 144 \t reward: 458\n",
      "Episode 18000 \t avg length: 144 \t reward: 472\n",
      "Episode 18500 \t avg length: 145 \t reward: 486\n",
      "Episode 19000 \t avg length: 144 \t reward: 434\n",
      "Episode 19500 \t avg length: 146 \t reward: 482\n",
      "Episode 20000 \t avg length: 149 \t reward: 552\n",
      "Episode 20500 \t avg length: 150 \t reward: 591\n",
      "Episode 21000 \t avg length: 149 \t reward: 589\n",
      "Episode 21500 \t avg length: 149 \t reward: 598\n",
      "Episode 22000 \t avg length: 152 \t reward: 674\n",
      "Episode 22500 \t avg length: 150 \t reward: 598\n",
      "Episode 23000 \t avg length: 149 \t reward: 569\n",
      "Episode 23500 \t avg length: 151 \t reward: 581\n",
      "Episode 24000 \t avg length: 151 \t reward: 638\n",
      "Episode 24500 \t avg length: 153 \t reward: 706\n",
      "Episode 25000 \t avg length: 154 \t reward: 711\n",
      "Episode 25500 \t avg length: 155 \t reward: 742\n",
      "Episode 26000 \t avg length: 155 \t reward: 686\n",
      "Episode 26500 \t avg length: 157 \t reward: 815\n",
      "Episode 27000 \t avg length: 156 \t reward: 752\n",
      "Episode 27500 \t avg length: 156 \t reward: 818\n",
      "Episode 28000 \t avg length: 159 \t reward: 861\n",
      "Episode 28500 \t avg length: 159 \t reward: 869\n",
      "Episode 29000 \t avg length: 157 \t reward: 742\n",
      "Episode 29500 \t avg length: 158 \t reward: 732\n",
      "Episode 30000 \t avg length: 158 \t reward: 815\n",
      "Episode 30500 \t avg length: 155 \t reward: 735\n",
      "Episode 31000 \t avg length: 156 \t reward: 781\n",
      "Episode 31500 \t avg length: 157 \t reward: 686\n",
      "Episode 32000 \t avg length: 158 \t reward: 695\n",
      "Episode 32500 \t avg length: 159 \t reward: 720\n",
      "Episode 33000 \t avg length: 159 \t reward: 753\n",
      "Episode 33500 \t avg length: 161 \t reward: 836\n",
      "Episode 34000 \t avg length: 160 \t reward: 854\n",
      "Episode 34500 \t avg length: 162 \t reward: 926\n",
      "Episode 35000 \t avg length: 163 \t reward: 834\n",
      "Episode 35500 \t avg length: 164 \t reward: 890\n",
      "Episode 36000 \t avg length: 161 \t reward: 805\n",
      "Episode 36500 \t avg length: 161 \t reward: 859\n",
      "Episode 37000 \t avg length: 160 \t reward: 847\n",
      "Episode 37500 \t avg length: 160 \t reward: 874\n",
      "Episode 38000 \t avg length: 163 \t reward: 908\n",
      "Episode 38500 \t avg length: 162 \t reward: 883\n",
      "Episode 39000 \t avg length: 163 \t reward: 996\n",
      "Episode 39500 \t avg length: 162 \t reward: 973\n",
      "Episode 40000 \t avg length: 164 \t reward: 1025\n",
      "Episode 40500 \t avg length: 162 \t reward: 952\n",
      "Episode 41000 \t avg length: 165 \t reward: 967\n",
      "Episode 41500 \t avg length: 166 \t reward: 964\n",
      "Episode 42000 \t avg length: 164 \t reward: 946\n",
      "Episode 42500 \t avg length: 166 \t reward: 1068\n",
      "Episode 43000 \t avg length: 165 \t reward: 1087\n",
      "Episode 43500 \t avg length: 165 \t reward: 1061\n",
      "Episode 44000 \t avg length: 165 \t reward: 1090\n",
      "Episode 44500 \t avg length: 165 \t reward: 1033\n",
      "Episode 45000 \t avg length: 167 \t reward: 1065\n",
      "Episode 45500 \t avg length: 168 \t reward: 986\n",
      "Episode 46000 \t avg length: 168 \t reward: 1004\n",
      "Episode 46500 \t avg length: 166 \t reward: 948\n",
      "Episode 47000 \t avg length: 168 \t reward: 1039\n",
      "Episode 47500 \t avg length: 163 \t reward: 932\n",
      "Episode 48000 \t avg length: 163 \t reward: 931\n",
      "Episode 48500 \t avg length: 164 \t reward: 974\n",
      "Episode 49000 \t avg length: 167 \t reward: 948\n",
      "Episode 49500 \t avg length: 164 \t reward: 988\n",
      "Episode 50000 \t avg length: 163 \t reward: 919\n",
      "Episode 50500 \t avg length: 161 \t reward: 948\n",
      "Episode 51000 \t avg length: 163 \t reward: 982\n",
      "Episode 51500 \t avg length: 164 \t reward: 980\n",
      "Episode 52000 \t avg length: 166 \t reward: 931\n",
      "Episode 52500 \t avg length: 163 \t reward: 966\n",
      "Episode 53000 \t avg length: 164 \t reward: 990\n",
      "Episode 53500 \t avg length: 166 \t reward: 989\n",
      "Episode 54000 \t avg length: 170 \t reward: 1067\n",
      "Episode 54500 \t avg length: 170 \t reward: 1029\n",
      "Episode 55000 \t avg length: 168 \t reward: 1058\n",
      "Episode 55500 \t avg length: 169 \t reward: 1115\n",
      "Episode 56000 \t avg length: 168 \t reward: 1060\n",
      "Episode 56500 \t avg length: 164 \t reward: 1023\n",
      "Episode 57000 \t avg length: 167 \t reward: 1005\n",
      "Episode 57500 \t avg length: 165 \t reward: 986\n",
      "Episode 58000 \t avg length: 167 \t reward: 1080\n",
      "Episode 58500 \t avg length: 163 \t reward: 984\n",
      "Episode 59000 \t avg length: 166 \t reward: 1084\n",
      "Episode 59500 \t avg length: 165 \t reward: 1099\n",
      "Episode 60000 \t avg length: 168 \t reward: 1077\n",
      "Episode 60500 \t avg length: 170 \t reward: 1059\n",
      "Episode 61000 \t avg length: 169 \t reward: 1024\n",
      "Episode 61500 \t avg length: 170 \t reward: 1028\n",
      "Episode 62000 \t avg length: 166 \t reward: 987\n",
      "Episode 62500 \t avg length: 168 \t reward: 957\n",
      "Episode 63000 \t avg length: 166 \t reward: 1005\n",
      "Episode 63500 \t avg length: 166 \t reward: 1061\n",
      "Episode 64000 \t avg length: 166 \t reward: 1045\n",
      "--- Time take to train model = 103.0 minutes ---\n"
     ]
    }
   ],
   "source": [
    "trainPPO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eUE1qOV9MN3M"
   },
   "outputs": [],
   "source": [
    "def testPPO(seed):\n",
    "    ############## Hyperparameters ##############\n",
    "    # creating environment\n",
    "    env, obs = initEnv(seed=seed)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    n_latent_var = 64           # number of variables in hidden layer\n",
    "    filename = \"./PPO_diner-dash_1e+07.pth\" # path to saved model\n",
    "    lr = 0.0007\n",
    "    betas = (0.9, 0.999)\n",
    "    gamma = 0.99                # discount factor\n",
    "    K_epochs = 4                # update policy for K epochs\n",
    "    eps_clip = 0.2              # clip parameter for PPO\n",
    "    #############################################\n",
    "    \n",
    "    memory = Memory()\n",
    "    ppo = PPO(state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip)\n",
    "    \n",
    "    ppo.policy_old.load_state_dict(torch.load(filename))\n",
    "\n",
    "    ep_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "      action = ppo.policy_old.act(obs, memory)\n",
    "      obs, reward, done, _ = env.step(action)\n",
    "      ep_reward += reward\n",
    "\n",
    "    actionList = [action.item() for action in memory.actions]\n",
    "\n",
    "    return ep_reward, actionList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v9AaOurkA68o"
   },
   "source": [
    "## [Stable Baselines](https://github.com/hill-a/stable-baselines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UhlZv0u3Wyn6"
   },
   "source": [
    "### Check Env setup for Stable Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "fPuA_s1cyeC6",
    "outputId": "a81e2082-d43c-42ef-e552-ecdfad717b46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CkerSMaSym9l",
    "outputId": "f9ddcf4d-b879-4ff6-8147-39c02c9148a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diner Dash environment is compatible with Stable-Baselines!\n"
     ]
    }
   ],
   "source": [
    "error = check_env(gym.make('diner_dash:DinerDash-v0').unwrapped)\n",
    "if error == None:\n",
    "  print(\"Diner Dash environment is compatible with Stable-Baselines!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DZN2HqxQXU5j"
   },
   "source": [
    "### Saving Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mgzEa-0CFK0H"
   },
   "source": [
    "#### Using Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PJSvrc-hGlz-"
   },
   "source": [
    "- Save a checkpoint every 1000 steps\n",
    "- Please change the callback variable name and name_prefix to whatever desire/specific to model\n",
    "- Model saved in ./logs directory\n",
    "\n",
    "  `PPO_callback = CheckpointCallback(save_freq=1000, save_path='./logs/', name_prefix='diner-dash-PPO')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sHqMaCOzFM2X"
   },
   "source": [
    "#### Using save function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5fagmv0RGGGq"
   },
   "source": [
    "- Model saved in current directory\n",
    "\n",
    "  `model.save('name-of-model')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fUaGTTZKHDCV"
   },
   "source": [
    "### Loading Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "__TWINPkHMMM"
   },
   "source": [
    "- If only for evaluation\n",
    "\n",
    "  `PPO_model = PPO2.load('name-of-model')`\n",
    "\n",
    "  `PPO_model.predict(state)`\n",
    "\n",
    "- If loading for further training\n",
    "\n",
    "  `PPO_model = PPO2.load('name-of-model', env)`\n",
    "\n",
    "  `PPO_model.learn(5000)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kITom2RTpWSK"
   },
   "source": [
    "### Wrapper for better training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B1WOnn4XpdYB"
   },
   "outputs": [],
   "source": [
    "class OneHotWrapper(gym.Wrapper):\n",
    "  \"\"\"\n",
    "  :param env: (gym.Env) Gym environment that will be wrapped\n",
    "  \"\"\"\n",
    "  def __init__(self, env):\n",
    "    # Call the parent constructor, so we can access self.env later\n",
    "    super(OneHotWrapper, self).__init__(env)\n",
    "    self.config = [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, \n",
    "                   2, 2, 2, 2, 2, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, \n",
    "                   7, 7, 19, 19]\n",
    "    self.low_state = np.array([-20] * sum(self.config), dtype=np.float32)\n",
    "    self.high_state = np.array([20] * sum(self.config), dtype=np.float32)\n",
    "    self.observation_space = gym.spaces.Box(low=self.low_state, high=self.high_state, dtype=np.float32)\n",
    "\n",
    "  def oneHotEncode(self, rawObs):\n",
    "    for i, val in enumerate(rawObs):\n",
    "      tmp = np.zeros(self.config[i])\n",
    "      tmp[val] = 1\n",
    "      if i == 0:\n",
    "        obs = tmp\n",
    "      else:\n",
    "        obs = np.concatenate((obs, tmp))\n",
    "    return obs\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"\n",
    "    Reset the environment \n",
    "    \"\"\"\n",
    "    obs = self.env.reset()\n",
    "    return self.oneHotEncode(obs)\n",
    "\n",
    "  def step(self, action):\n",
    "    \"\"\"\n",
    "    :param action: ([float] or int) Action taken by the agent\n",
    "    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
    "    \"\"\"\n",
    "    obs, reward, done, info = self.env.step(action)\n",
    "    return self.oneHotEncode(obs), reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RWruL1pM4X5H"
   },
   "source": [
    "### PPO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Mq_Cj8iR0Rqx",
    "outputId": "d075610d-f9db-48f5-d408-d8306540aaaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:198: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:240: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "--------------------------------------\n",
      "| approxkl           | 8.491728e-06  |\n",
      "| clipfrac           | 0.0           |\n",
      "| ep_len_mean        | 113           |\n",
      "| ep_reward_mean     | -1.36e+03     |\n",
      "| explained_variance | -0.000222     |\n",
      "| fps                | 1196          |\n",
      "| n_updates          | 1             |\n",
      "| policy_entropy     | 4.043032      |\n",
      "| policy_loss        | -0.0006455337 |\n",
      "| serial_timesteps   | 128           |\n",
      "| time_elapsed       | 3.62e-05      |\n",
      "| total_timesteps    | 3072          |\n",
      "| value_loss         | 24675.428     |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-4d00fadbf1e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mppo2_callback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"--- Time take to train model = {(time.time() - start_time)//60} minutes ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_rollout_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0;31m# true_reward is the reward without discount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0mrollout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0;31m# Unpack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneglogpacs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrollout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stable_baselines/common/runners.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callback)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m                 \u001b[0mclipped_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stable_baselines/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \"\"\"\n\u001b[1;32m    149\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stable_baselines/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_rews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_infos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0;31m# save final observation where user can get it, then reset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-986a99a5e074>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \"\"\"\n\u001b[1;32m     37\u001b[0m     \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moneHotEncode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-986a99a5e074>\u001b[0m in \u001b[0;36moneHotEncode\u001b[0;34m(self, rawObs)\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0moneHotEncode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawObs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrawObs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m       \u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Refer to the stable baseline documentation for alternative implementations\n",
    "# of callbacks, baselines and others\n",
    "from stable_baselines.common.callbacks import CheckpointCallback\n",
    "from stable_baselines.common import make_vec_env\n",
    "from stable_baselines import PPO2\n",
    "\n",
    "# fixed params for challenge\n",
    "# Total timesteps for training\n",
    "tts = int(1e7) \n",
    "\n",
    "# custom params\n",
    "n_envs = 24\n",
    "save_freq = int(1e5)\n",
    "log_interval = 500\n",
    "\n",
    "# Vectorize environment\n",
    "env = make_vec_env(env_id=\"diner_dash:DinerDash-v0\", n_envs=n_envs, wrapper_class=OneHotWrapper)\n",
    "\n",
    "# Initialise model\n",
    "model = PPO2('MlpPolicy', env, verbose=1)\n",
    "# model = A2C(CnnPolicy, env, lr_schedule='constant', verbose=1)\n",
    "# model.learn(total_timesteps=int(5e6))\n",
    "\n",
    "# Initialise callback\n",
    "ppo2_callback = CheckpointCallback(save_freq=save_freq, save_path=f'./logs-PPO2-nenv={n_envs}-tts={tts:.0e}/', name_prefix='diner-dash-PPO')\n",
    "\n",
    "# Train model\n",
    "start_time = time.time()\n",
    "model.learn(total_timesteps=tts, log_interval=log_interval, callback=ppo2_callback)\n",
    "print(f\"--- Time take to train model = {(time.time() - start_time)//60} minutes ---\")\n",
    "\n",
    "# Save model\n",
    "print(\"Saving Final Model...\")\n",
    "modelDirectory = \"./\"\n",
    "modelName = f\"PPO2-nenv={n_envs}-tts={tts:.0e}\"\n",
    "model.save(modelDirectory + modelName)\n",
    "print(f\"Model saved as {modelDirectory + modelName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6AhDY9hj1rht"
   },
   "outputs": [],
   "source": [
    "def testPPO2(seed):\n",
    "  from stable_baselines.common import make_vec_env\n",
    "  from stable_baselines import PPO2\n",
    "\n",
    "  # Vectorize environment with given seed\n",
    "  env = make_vec_env(env_id=\"diner_dash:DinerDash-v0\", wrapper_class=OneHotWrapper, seed=seed)\n",
    "\n",
    "  # Load saved model\n",
    "  PPO_model = PPO2.load(\"PPO2-nenv=24-tts=1e+07\", env=env)\n",
    "\n",
    "  # Reset environment, init obs\n",
    "  obs = env.reset()\n",
    "\n",
    "  done = False\n",
    "  sum_rewards = 0\n",
    "  action_list = []\n",
    "\n",
    "  while not done:\n",
    "    action, _states = PPO_model.predict(obs)\n",
    "    action_list.append(action.item())\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    sum_rewards += rewards\n",
    "\n",
    "  return sum_rewards, action_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yP6UoOZKFKYZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NTZ7eJIipTsj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PnWUVi76FTta"
   },
   "source": [
    "### ACER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "2me2pFEIpVTz",
    "outputId": "94bbf271-1152-459b-e990-e37ca7688ae2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/acer/acer_simple.py:454: The name tf.global_norm is deprecated. Please use tf.linalg.global_norm instead.\n",
      "\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 0.000288 |\n",
      "| avg_norm_g          | 404      |\n",
      "| avg_norm_grads_f    | 404      |\n",
      "| avg_norm_k          | 7.55     |\n",
      "| avg_norm_k_dot_g    | 404      |\n",
      "| entropy             | 2.04e+03 |\n",
      "| explained_variance  | 6.5e-06  |\n",
      "| fps                 | 0        |\n",
      "| loss                | 36.3     |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 28.6     |\n",
      "| loss_policy         | 28.6     |\n",
      "| loss_q              | 56.2     |\n",
      "| mean_episode_length | 0        |\n",
      "| mean_episode_reward | 0        |\n",
      "| norm_grads          | 3.18     |\n",
      "| norm_grads_policy   | 2.36     |\n",
      "| norm_grads_q        | 2.13     |\n",
      "| total_timesteps     | 480      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 47.3     |\n",
      "| avg_norm_g          | 4.61e+03 |\n",
      "| avg_norm_grads_f    | 4.58e+03 |\n",
      "| avg_norm_k          | 7.74     |\n",
      "| avg_norm_k_dot_g    | 4.59e+03 |\n",
      "| entropy             | 1.4e+03  |\n",
      "| explained_variance  | 0.000737 |\n",
      "| fps                 | 3129     |\n",
      "| loss                | 8.63e+03 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -111     |\n",
      "| loss_policy         | -111     |\n",
      "| loss_q              | 1.75e+04 |\n",
      "| mean_episode_length | 143      |\n",
      "| mean_episode_reward | -720     |\n",
      "| norm_grads          | 90.4     |\n",
      "| norm_grads_policy   | 58.3     |\n",
      "| norm_grads_q        | 69       |\n",
      "| total_timesteps     | 240480   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 19.1     |\n",
      "| avg_norm_g          | 5.28e+03 |\n",
      "| avg_norm_grads_f    | 5.27e+03 |\n",
      "| avg_norm_k          | 6.91     |\n",
      "| avg_norm_k_dot_g    | 4.51e+03 |\n",
      "| entropy             | 963      |\n",
      "| explained_variance  | 0.345    |\n",
      "| fps                 | 3089     |\n",
      "| loss                | 3.84e+03 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 26.5     |\n",
      "| loss_policy         | 26.5     |\n",
      "| loss_q              | 7.65e+03 |\n",
      "| mean_episode_length | 168      |\n",
      "| mean_episode_reward | 419      |\n",
      "| norm_grads          | 4.38e+03 |\n",
      "| norm_grads_policy   | 57.4     |\n",
      "| norm_grads_q        | 4.38e+03 |\n",
      "| total_timesteps     | 480480   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 14.2     |\n",
      "| avg_norm_g          | 2.88e+03 |\n",
      "| avg_norm_grads_f    | 2.87e+03 |\n",
      "| avg_norm_k          | 8.08     |\n",
      "| avg_norm_k_dot_g    | 3.05e+03 |\n",
      "| entropy             | 861      |\n",
      "| explained_variance  | 0.475    |\n",
      "| fps                 | 3083     |\n",
      "| loss                | 4.49e+03 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 55.5     |\n",
      "| loss_policy         | 55.5     |\n",
      "| loss_q              | 8.88e+03 |\n",
      "| mean_episode_length | 190      |\n",
      "| mean_episode_reward | 1.1e+03  |\n",
      "| norm_grads          | 1.77e+03 |\n",
      "| norm_grads_policy   | 62.2     |\n",
      "| norm_grads_q        | 1.77e+03 |\n",
      "| total_timesteps     | 720480   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 54       |\n",
      "| avg_norm_g          | 1.03e+04 |\n",
      "| avg_norm_grads_f    | 1.03e+04 |\n",
      "| avg_norm_k          | 9.57     |\n",
      "| avg_norm_k_dot_g    | 1.23e+04 |\n",
      "| entropy             | 634      |\n",
      "| explained_variance  | 0.352    |\n",
      "| fps                 | 3084     |\n",
      "| loss                | 4.87e+03 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 109      |\n",
      "| loss_policy         | 109      |\n",
      "| loss_q              | 9.53e+03 |\n",
      "| mean_episode_length | 214      |\n",
      "| mean_episode_reward | 2.01e+03 |\n",
      "| norm_grads          | 1.36e+03 |\n",
      "| norm_grads_policy   | 75.1     |\n",
      "| norm_grads_q        | 1.36e+03 |\n",
      "| total_timesteps     | 960480   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 22.9     |\n",
      "| avg_norm_g          | 7.74e+03 |\n",
      "| avg_norm_grads_f    | 7.73e+03 |\n",
      "| avg_norm_k          | 8.68     |\n",
      "| avg_norm_k_dot_g    | 8.99e+03 |\n",
      "| entropy             | 651      |\n",
      "| explained_variance  | 0.123    |\n",
      "| fps                 | 3079     |\n",
      "| loss                | 5.87e+03 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 128      |\n",
      "| loss_policy         | 128      |\n",
      "| loss_q              | 1.15e+04 |\n",
      "| mean_episode_length | 223      |\n",
      "| mean_episode_reward | 2.17e+03 |\n",
      "| norm_grads          | 2.63e+03 |\n",
      "| norm_grads_policy   | 90.7     |\n",
      "| norm_grads_q        | 2.63e+03 |\n",
      "| total_timesteps     | 1200480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 15.9     |\n",
      "| avg_norm_g          | 2.74e+03 |\n",
      "| avg_norm_grads_f    | 2.74e+03 |\n",
      "| avg_norm_k          | 6.56     |\n",
      "| avg_norm_k_dot_g    | 2.4e+03  |\n",
      "| entropy             | 446      |\n",
      "| explained_variance  | 0.04     |\n",
      "| fps                 | 3084     |\n",
      "| loss                | 1.36e+04 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 23.7     |\n",
      "| loss_policy         | 23.7     |\n",
      "| loss_q              | 2.71e+04 |\n",
      "| mean_episode_length | 255      |\n",
      "| mean_episode_reward | 3.01e+03 |\n",
      "| norm_grads          | 679      |\n",
      "| norm_grads_policy   | 150      |\n",
      "| norm_grads_q        | 663      |\n",
      "| total_timesteps     | 1440480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 80.4     |\n",
      "| avg_norm_g          | 6.5e+03  |\n",
      "| avg_norm_grads_f    | 6.44e+03 |\n",
      "| avg_norm_k          | 7.74     |\n",
      "| avg_norm_k_dot_g    | 8.98e+03 |\n",
      "| entropy             | 452      |\n",
      "| explained_variance  | -0.0381  |\n",
      "| fps                 | 3082     |\n",
      "| loss                | 7.18e+03 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 96.1     |\n",
      "| loss_policy         | 96.1     |\n",
      "| loss_q              | 1.42e+04 |\n",
      "| mean_episode_length | 300      |\n",
      "| mean_episode_reward | 4.02e+03 |\n",
      "| norm_grads          | 1.69e+03 |\n",
      "| norm_grads_policy   | 107      |\n",
      "| norm_grads_q        | 1.69e+03 |\n",
      "| total_timesteps     | 1680480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 3.7      |\n",
      "| avg_norm_g          | 2.11e+03 |\n",
      "| avg_norm_grads_f    | 2.11e+03 |\n",
      "| avg_norm_k          | 7.46     |\n",
      "| avg_norm_k_dot_g    | 2.3e+03  |\n",
      "| entropy             | 395      |\n",
      "| explained_variance  | 0.00054  |\n",
      "| fps                 | 3082     |\n",
      "| loss                | 9.75e+03 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 84.9     |\n",
      "| loss_policy         | 84.9     |\n",
      "| loss_q              | 1.93e+04 |\n",
      "| mean_episode_length | 314      |\n",
      "| mean_episode_reward | 4.22e+03 |\n",
      "| norm_grads          | 898      |\n",
      "| norm_grads_policy   | 99.8     |\n",
      "| norm_grads_q        | 892      |\n",
      "| total_timesteps     | 1920480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 3.51     |\n",
      "| avg_norm_g          | 3.94e+03 |\n",
      "| avg_norm_grads_f    | 3.94e+03 |\n",
      "| avg_norm_k          | 6.35     |\n",
      "| avg_norm_k_dot_g    | 3.56e+03 |\n",
      "| entropy             | 369      |\n",
      "| explained_variance  | 0.202    |\n",
      "| fps                 | 3086     |\n",
      "| loss                | 7.05e+03 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 72       |\n",
      "| loss_policy         | 72       |\n",
      "| loss_q              | 1.4e+04  |\n",
      "| mean_episode_length | 346      |\n",
      "| mean_episode_reward | 4.96e+03 |\n",
      "| norm_grads          | 848      |\n",
      "| norm_grads_policy   | 116      |\n",
      "| norm_grads_q        | 840      |\n",
      "| total_timesteps     | 2160480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 9.23     |\n",
      "| avg_norm_g          | 1.88e+03 |\n",
      "| avg_norm_grads_f    | 1.87e+03 |\n",
      "| avg_norm_k          | 6.75     |\n",
      "| avg_norm_k_dot_g    | 1.97e+03 |\n",
      "| entropy             | 374      |\n",
      "| explained_variance  | -0.0117  |\n",
      "| fps                 | 3088     |\n",
      "| loss                | 9.15e+03 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 45.7     |\n",
      "| loss_policy         | 45.7     |\n",
      "| loss_q              | 1.82e+04 |\n",
      "| mean_episode_length | 340      |\n",
      "| mean_episode_reward | 4.87e+03 |\n",
      "| norm_grads          | 634      |\n",
      "| norm_grads_policy   | 110      |\n",
      "| norm_grads_q        | 625      |\n",
      "| total_timesteps     | 2400480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 918      |\n",
      "| avg_norm_g          | 1.54e+04 |\n",
      "| avg_norm_grads_f    | 1.49e+04 |\n",
      "| avg_norm_k          | 7.12     |\n",
      "| avg_norm_k_dot_g    | 1.48e+04 |\n",
      "| entropy             | 309      |\n",
      "| explained_variance  | -0.0222  |\n",
      "| fps                 | 3088     |\n",
      "| loss                | 6.62e+03 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 53.4     |\n",
      "| loss_policy         | 53.4     |\n",
      "| loss_q              | 1.31e+04 |\n",
      "| mean_episode_length | 387      |\n",
      "| mean_episode_reward | 5.97e+03 |\n",
      "| norm_grads          | 1.3e+03  |\n",
      "| norm_grads_policy   | 120      |\n",
      "| norm_grads_q        | 1.3e+03  |\n",
      "| total_timesteps     | 2640480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 4.5      |\n",
      "| avg_norm_g          | 7.88e+03 |\n",
      "| avg_norm_grads_f    | 7.88e+03 |\n",
      "| avg_norm_k          | 8.11     |\n",
      "| avg_norm_k_dot_g    | 1.07e+04 |\n",
      "| entropy             | 312      |\n",
      "| explained_variance  | 0.172    |\n",
      "| fps                 | 3088     |\n",
      "| loss                | 8.31e+03 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 60.4     |\n",
      "| loss_policy         | 60.4     |\n",
      "| loss_q              | 1.65e+04 |\n",
      "| mean_episode_length | 351      |\n",
      "| mean_episode_reward | 5.18e+03 |\n",
      "| norm_grads          | 4.21e+03 |\n",
      "| norm_grads_policy   | 127      |\n",
      "| norm_grads_q        | 4.21e+03 |\n",
      "| total_timesteps     | 2880480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 1.63     |\n",
      "| avg_norm_g          | 2.39e+03 |\n",
      "| avg_norm_grads_f    | 2.39e+03 |\n",
      "| avg_norm_k          | 5.85     |\n",
      "| avg_norm_k_dot_g    | 2.57e+03 |\n",
      "| entropy             | 281      |\n",
      "| explained_variance  | -0.0557  |\n",
      "| fps                 | 3087     |\n",
      "| loss                | 7.52e+03 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 62.8     |\n",
      "| loss_policy         | 62.8     |\n",
      "| loss_q              | 1.49e+04 |\n",
      "| mean_episode_length | 378      |\n",
      "| mean_episode_reward | 5.87e+03 |\n",
      "| norm_grads          | 1.96e+03 |\n",
      "| norm_grads_policy   | 73.2     |\n",
      "| norm_grads_q        | 1.96e+03 |\n",
      "| total_timesteps     | 3120480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 3.5      |\n",
      "| avg_norm_g          | 1.54e+03 |\n",
      "| avg_norm_grads_f    | 1.54e+03 |\n",
      "| avg_norm_k          | 7.12     |\n",
      "| avg_norm_k_dot_g    | 1.45e+03 |\n",
      "| entropy             | 275      |\n",
      "| explained_variance  | -0.0692  |\n",
      "| fps                 | 3085     |\n",
      "| loss                | 1.37e+04 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 49.6     |\n",
      "| loss_policy         | 49.6     |\n",
      "| loss_q              | 2.73e+04 |\n",
      "| mean_episode_length | 374      |\n",
      "| mean_episode_reward | 5.73e+03 |\n",
      "| norm_grads          | 2.66e+03 |\n",
      "| norm_grads_policy   | 217      |\n",
      "| norm_grads_q        | 2.65e+03 |\n",
      "| total_timesteps     | 3360480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 3.67     |\n",
      "| avg_norm_g          | 1.56e+03 |\n",
      "| avg_norm_grads_f    | 1.56e+03 |\n",
      "| avg_norm_k          | 7.03     |\n",
      "| avg_norm_k_dot_g    | 1.74e+03 |\n",
      "| entropy             | 300      |\n",
      "| explained_variance  | 0.0381   |\n",
      "| fps                 | 3084     |\n",
      "| loss                | 1.01e+04 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 26.1     |\n",
      "| loss_policy         | 26.1     |\n",
      "| loss_q              | 2.03e+04 |\n",
      "| mean_episode_length | 394      |\n",
      "| mean_episode_reward | 6.22e+03 |\n",
      "| norm_grads          | 2.17e+03 |\n",
      "| norm_grads_policy   | 107      |\n",
      "| norm_grads_q        | 2.17e+03 |\n",
      "| total_timesteps     | 3600480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 3.22     |\n",
      "| avg_norm_g          | 9.31e+03 |\n",
      "| avg_norm_grads_f    | 9.3e+03  |\n",
      "| avg_norm_k          | 6.1      |\n",
      "| avg_norm_k_dot_g    | 8.03e+03 |\n",
      "| entropy             | 291      |\n",
      "| explained_variance  | -0.0416  |\n",
      "| fps                 | 3084     |\n",
      "| loss                | 1.25e+04 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 85.6     |\n",
      "| loss_policy         | 85.6     |\n",
      "| loss_q              | 2.49e+04 |\n",
      "| mean_episode_length | 431      |\n",
      "| mean_episode_reward | 7e+03    |\n",
      "| norm_grads          | 6.26e+03 |\n",
      "| norm_grads_policy   | 235      |\n",
      "| norm_grads_q        | 6.25e+03 |\n",
      "| total_timesteps     | 3840480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 712      |\n",
      "| avg_norm_g          | 3.66e+04 |\n",
      "| avg_norm_grads_f    | 3.62e+04 |\n",
      "| avg_norm_k          | 5.92     |\n",
      "| avg_norm_k_dot_g    | 3.19e+04 |\n",
      "| entropy             | 289      |\n",
      "| explained_variance  | 0.122    |\n",
      "| fps                 | 3084     |\n",
      "| loss                | 1.43e+04 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -8.95    |\n",
      "| loss_policy         | -8.95    |\n",
      "| loss_q              | 2.87e+04 |\n",
      "| mean_episode_length | 412      |\n",
      "| mean_episode_reward | 6.69e+03 |\n",
      "| norm_grads          | 3.32e+03 |\n",
      "| norm_grads_policy   | 847      |\n",
      "| norm_grads_q        | 3.21e+03 |\n",
      "| total_timesteps     | 4080480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 15.3     |\n",
      "| avg_norm_g          | 3.45e+03 |\n",
      "| avg_norm_grads_f    | 3.44e+03 |\n",
      "| avg_norm_k          | 6.01     |\n",
      "| avg_norm_k_dot_g    | 3.56e+03 |\n",
      "| entropy             | 274      |\n",
      "| explained_variance  | 0.375    |\n",
      "| fps                 | 3084     |\n",
      "| loss                | 1.22e+04 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 25.3     |\n",
      "| loss_policy         | 25.3     |\n",
      "| loss_q              | 2.44e+04 |\n",
      "| mean_episode_length | 408      |\n",
      "| mean_episode_reward | 6.55e+03 |\n",
      "| norm_grads          | 4.42e+03 |\n",
      "| norm_grads_policy   | 135      |\n",
      "| norm_grads_q        | 4.42e+03 |\n",
      "| total_timesteps     | 4320480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 12.2     |\n",
      "| avg_norm_g          | 1.16e+03 |\n",
      "| avg_norm_grads_f    | 1.16e+03 |\n",
      "| avg_norm_k          | 5.88     |\n",
      "| avg_norm_k_dot_g    | 1.24e+03 |\n",
      "| entropy             | 241      |\n",
      "| explained_variance  | 0.303    |\n",
      "| fps                 | 3084     |\n",
      "| loss                | 1.63e+04 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 4.75     |\n",
      "| loss_policy         | 4.75     |\n",
      "| loss_q              | 3.27e+04 |\n",
      "| mean_episode_length | 427      |\n",
      "| mean_episode_reward | 6.94e+03 |\n",
      "| norm_grads          | 1.38e+04 |\n",
      "| norm_grads_policy   | 142      |\n",
      "| norm_grads_q        | 1.38e+04 |\n",
      "| total_timesteps     | 4560480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 4.16     |\n",
      "| avg_norm_g          | 4.28e+03 |\n",
      "| avg_norm_grads_f    | 4.28e+03 |\n",
      "| avg_norm_k          | 5.26     |\n",
      "| avg_norm_k_dot_g    | 3.45e+03 |\n",
      "| entropy             | 224      |\n",
      "| explained_variance  | 0.213    |\n",
      "| fps                 | 3083     |\n",
      "| loss                | 1.05e+04 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 13.6     |\n",
      "| loss_policy         | 13.6     |\n",
      "| loss_q              | 2.11e+04 |\n",
      "| mean_episode_length | 394      |\n",
      "| mean_episode_reward | 6.14e+03 |\n",
      "| norm_grads          | 4.61e+03 |\n",
      "| norm_grads_policy   | 118      |\n",
      "| norm_grads_q        | 4.61e+03 |\n",
      "| total_timesteps     | 4800480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 15.5     |\n",
      "| avg_norm_g          | 1.96e+03 |\n",
      "| avg_norm_grads_f    | 1.95e+03 |\n",
      "| avg_norm_k          | 5.28     |\n",
      "| avg_norm_k_dot_g    | 1.85e+03 |\n",
      "| entropy             | 246      |\n",
      "| explained_variance  | 0.106    |\n",
      "| fps                 | 3083     |\n",
      "| loss                | 2.55e+04 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 25.4     |\n",
      "| loss_policy         | 25.4     |\n",
      "| loss_q              | 5.1e+04  |\n",
      "| mean_episode_length | 421      |\n",
      "| mean_episode_reward | 6.66e+03 |\n",
      "| norm_grads          | 2.76e+03 |\n",
      "| norm_grads_policy   | 159      |\n",
      "| norm_grads_q        | 2.75e+03 |\n",
      "| total_timesteps     | 5040480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 272      |\n",
      "| avg_norm_g          | 1.09e+04 |\n",
      "| avg_norm_grads_f    | 1.06e+04 |\n",
      "| avg_norm_k          | 5.58     |\n",
      "| avg_norm_k_dot_g    | 1.67e+04 |\n",
      "| entropy             | 249      |\n",
      "| explained_variance  | -0.0352  |\n",
      "| fps                 | 3083     |\n",
      "| loss                | 2.15e+04 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -17.4    |\n",
      "| loss_policy         | -17.4    |\n",
      "| loss_q              | 4.3e+04  |\n",
      "| mean_episode_length | 461      |\n",
      "| mean_episode_reward | 7.83e+03 |\n",
      "| norm_grads          | 7.37e+03 |\n",
      "| norm_grads_policy   | 792      |\n",
      "| norm_grads_q        | 7.33e+03 |\n",
      "| total_timesteps     | 5280480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 6.15     |\n",
      "| avg_norm_g          | 1.19e+03 |\n",
      "| avg_norm_grads_f    | 1.19e+03 |\n",
      "| avg_norm_k          | 5.37     |\n",
      "| avg_norm_k_dot_g    | 1.15e+03 |\n",
      "| entropy             | 214      |\n",
      "| explained_variance  | 0.0981   |\n",
      "| fps                 | 3083     |\n",
      "| loss                | 2.12e+04 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 6.52     |\n",
      "| loss_policy         | 6.52     |\n",
      "| loss_q              | 4.24e+04 |\n",
      "| mean_episode_length | 426      |\n",
      "| mean_episode_reward | 7.02e+03 |\n",
      "| norm_grads          | 5.51e+03 |\n",
      "| norm_grads_policy   | 169      |\n",
      "| norm_grads_q        | 5.5e+03  |\n",
      "| total_timesteps     | 5520480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 15.3     |\n",
      "| avg_norm_g          | 2.42e+03 |\n",
      "| avg_norm_grads_f    | 2.41e+03 |\n",
      "| avg_norm_k          | 6.16     |\n",
      "| avg_norm_k_dot_g    | 2.41e+03 |\n",
      "| entropy             | 283      |\n",
      "| explained_variance  | 0.0388   |\n",
      "| fps                 | 3084     |\n",
      "| loss                | 3.07e+04 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 29       |\n",
      "| loss_policy         | 29       |\n",
      "| loss_q              | 6.13e+04 |\n",
      "| mean_episode_length | 399      |\n",
      "| mean_episode_reward | 6.29e+03 |\n",
      "| norm_grads          | 8.09e+03 |\n",
      "| norm_grads_policy   | 247      |\n",
      "| norm_grads_q        | 8.09e+03 |\n",
      "| total_timesteps     | 5760480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 2.11     |\n",
      "| avg_norm_g          | 4.51e+03 |\n",
      "| avg_norm_grads_f    | 4.51e+03 |\n",
      "| avg_norm_k          | 6.27     |\n",
      "| avg_norm_k_dot_g    | 4.7e+03  |\n",
      "| entropy             | 276      |\n",
      "| explained_variance  | 0.305    |\n",
      "| fps                 | 3084     |\n",
      "| loss                | 8.16e+03 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 32.1     |\n",
      "| loss_policy         | 32.1     |\n",
      "| loss_q              | 1.63e+04 |\n",
      "| mean_episode_length | 416      |\n",
      "| mean_episode_reward | 6.88e+03 |\n",
      "| norm_grads          | 5.06e+03 |\n",
      "| norm_grads_policy   | 123      |\n",
      "| norm_grads_q        | 5.05e+03 |\n",
      "| total_timesteps     | 6000480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 92.9     |\n",
      "| avg_norm_g          | 6.8e+03  |\n",
      "| avg_norm_grads_f    | 6.74e+03 |\n",
      "| avg_norm_k          | 5.45     |\n",
      "| avg_norm_k_dot_g    | 8.2e+03  |\n",
      "| entropy             | 234      |\n",
      "| explained_variance  | 0.19     |\n",
      "| fps                 | 3084     |\n",
      "| loss                | 1.58e+04 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 2.52     |\n",
      "| loss_policy         | 2.52     |\n",
      "| loss_q              | 3.16e+04 |\n",
      "| mean_episode_length | 440      |\n",
      "| mean_episode_reward | 7.33e+03 |\n",
      "| norm_grads          | 6.56e+03 |\n",
      "| norm_grads_policy   | 233      |\n",
      "| norm_grads_q        | 6.56e+03 |\n",
      "| total_timesteps     | 6240480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 0.62     |\n",
      "| avg_norm_g          | 477      |\n",
      "| avg_norm_grads_f    | 477      |\n",
      "| avg_norm_k          | 5.32     |\n",
      "| avg_norm_k_dot_g    | 593      |\n",
      "| entropy             | 216      |\n",
      "| explained_variance  | 0.229    |\n",
      "| fps                 | 3084     |\n",
      "| loss                | 2.95e+03 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 29.7     |\n",
      "| loss_policy         | 29.7     |\n",
      "| loss_q              | 5.85e+03 |\n",
      "| mean_episode_length | 472      |\n",
      "| mean_episode_reward | 8.03e+03 |\n",
      "| norm_grads          | 2.16e+03 |\n",
      "| norm_grads_policy   | 66.8     |\n",
      "| norm_grads_q        | 2.16e+03 |\n",
      "| total_timesteps     | 6480480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 2.27     |\n",
      "| avg_norm_g          | 1.7e+03  |\n",
      "| avg_norm_grads_f    | 1.7e+03  |\n",
      "| avg_norm_k          | 5.43     |\n",
      "| avg_norm_k_dot_g    | 1.54e+03 |\n",
      "| entropy             | 207      |\n",
      "| explained_variance  | 0.627    |\n",
      "| fps                 | 3085     |\n",
      "| loss                | 3.14e+03 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 24.1     |\n",
      "| loss_policy         | 24.1     |\n",
      "| loss_q              | 6.24e+03 |\n",
      "| mean_episode_length | 447      |\n",
      "| mean_episode_reward | 7.44e+03 |\n",
      "| norm_grads          | 1.46e+03 |\n",
      "| norm_grads_policy   | 67.6     |\n",
      "| norm_grads_q        | 1.45e+03 |\n",
      "| total_timesteps     | 6720480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 40.3     |\n",
      "| avg_norm_g          | 2.04e+03 |\n",
      "| avg_norm_grads_f    | 2.02e+03 |\n",
      "| avg_norm_k          | 5.12     |\n",
      "| avg_norm_k_dot_g    | 1.83e+03 |\n",
      "| entropy             | 227      |\n",
      "| explained_variance  | 0.388    |\n",
      "| fps                 | 3085     |\n",
      "| loss                | 9.85e+03 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 1.12     |\n",
      "| loss_policy         | 1.12     |\n",
      "| loss_q              | 1.97e+04 |\n",
      "| mean_episode_length | 425      |\n",
      "| mean_episode_reward | 7.03e+03 |\n",
      "| norm_grads          | 7.45e+03 |\n",
      "| norm_grads_policy   | 122      |\n",
      "| norm_grads_q        | 7.45e+03 |\n",
      "| total_timesteps     | 6960480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 3.93     |\n",
      "| avg_norm_g          | 652      |\n",
      "| avg_norm_grads_f    | 650      |\n",
      "| avg_norm_k          | 5.65     |\n",
      "| avg_norm_k_dot_g    | 724      |\n",
      "| entropy             | 231      |\n",
      "| explained_variance  | 0.299    |\n",
      "| fps                 | 3085     |\n",
      "| loss                | 1.29e+04 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 14.1     |\n",
      "| loss_policy         | 14.1     |\n",
      "| loss_q              | 2.58e+04 |\n",
      "| mean_episode_length | 418      |\n",
      "| mean_episode_reward | 6.75e+03 |\n",
      "| norm_grads          | 8.15e+03 |\n",
      "| norm_grads_policy   | 117      |\n",
      "| norm_grads_q        | 8.15e+03 |\n",
      "| total_timesteps     | 7200480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 3.73     |\n",
      "| avg_norm_g          | 1.07e+03 |\n",
      "| avg_norm_grads_f    | 1.06e+03 |\n",
      "| avg_norm_k          | 5.22     |\n",
      "| avg_norm_k_dot_g    | 1.12e+03 |\n",
      "| entropy             | 180      |\n",
      "| explained_variance  | 0.487    |\n",
      "| fps                 | 3086     |\n",
      "| loss                | 6.92e+03 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 7.07     |\n",
      "| loss_policy         | 7.07     |\n",
      "| loss_q              | 1.38e+04 |\n",
      "| mean_episode_length | 434      |\n",
      "| mean_episode_reward | 7.33e+03 |\n",
      "| norm_grads          | 6.32e+03 |\n",
      "| norm_grads_policy   | 121      |\n",
      "| norm_grads_q        | 6.32e+03 |\n",
      "| total_timesteps     | 7440480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 18.1     |\n",
      "| avg_norm_g          | 2.42e+03 |\n",
      "| avg_norm_grads_f    | 2.41e+03 |\n",
      "| avg_norm_k          | 5.38     |\n",
      "| avg_norm_k_dot_g    | 2.36e+03 |\n",
      "| entropy             | 200      |\n",
      "| explained_variance  | 0.39     |\n",
      "| fps                 | 3087     |\n",
      "| loss                | 1.63e+04 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 6.38     |\n",
      "| loss_policy         | 6.38     |\n",
      "| loss_q              | 3.26e+04 |\n",
      "| mean_episode_length | 459      |\n",
      "| mean_episode_reward | 7.66e+03 |\n",
      "| norm_grads          | 1.41e+04 |\n",
      "| norm_grads_policy   | 166      |\n",
      "| norm_grads_q        | 1.41e+04 |\n",
      "| total_timesteps     | 7680480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 6.73     |\n",
      "| avg_norm_g          | 6.79e+03 |\n",
      "| avg_norm_grads_f    | 6.79e+03 |\n",
      "| avg_norm_k          | 5.34     |\n",
      "| avg_norm_k_dot_g    | 6.59e+03 |\n",
      "| entropy             | 249      |\n",
      "| explained_variance  | 0.219    |\n",
      "| fps                 | 3088     |\n",
      "| loss                | 1.28e+04 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 41.7     |\n",
      "| loss_policy         | 41.7     |\n",
      "| loss_q              | 2.54e+04 |\n",
      "| mean_episode_length | 453      |\n",
      "| mean_episode_reward | 7.58e+03 |\n",
      "| norm_grads          | 6.57e+03 |\n",
      "| norm_grads_policy   | 212      |\n",
      "| norm_grads_q        | 6.56e+03 |\n",
      "| total_timesteps     | 7920480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 13.4     |\n",
      "| avg_norm_g          | 748      |\n",
      "| avg_norm_grads_f    | 742      |\n",
      "| avg_norm_k          | 5.4      |\n",
      "| avg_norm_k_dot_g    | 687      |\n",
      "| entropy             | 216      |\n",
      "| explained_variance  | 0.545    |\n",
      "| fps                 | 3092     |\n",
      "| loss                | 1.07e+04 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -14.5    |\n",
      "| loss_policy         | -14.5    |\n",
      "| loss_q              | 2.15e+04 |\n",
      "| mean_episode_length | 436      |\n",
      "| mean_episode_reward | 7.18e+03 |\n",
      "| norm_grads          | 1.05e+04 |\n",
      "| norm_grads_policy   | 128      |\n",
      "| norm_grads_q        | 1.05e+04 |\n",
      "| total_timesteps     | 8160480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 4.84     |\n",
      "| avg_norm_g          | 416      |\n",
      "| avg_norm_grads_f    | 413      |\n",
      "| avg_norm_k          | 5.06     |\n",
      "| avg_norm_k_dot_g    | 408      |\n",
      "| entropy             | 251      |\n",
      "| explained_variance  | 0.261    |\n",
      "| fps                 | 3095     |\n",
      "| loss                | 1.5e+04  |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 10.4     |\n",
      "| loss_policy         | 10.4     |\n",
      "| loss_q              | 3.01e+04 |\n",
      "| mean_episode_length | 476      |\n",
      "| mean_episode_reward | 8.17e+03 |\n",
      "| norm_grads          | 3.41e+03 |\n",
      "| norm_grads_policy   | 220      |\n",
      "| norm_grads_q        | 3.4e+03  |\n",
      "| total_timesteps     | 8400480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 87.5     |\n",
      "| avg_norm_g          | 5.03e+03 |\n",
      "| avg_norm_grads_f    | 4.99e+03 |\n",
      "| avg_norm_k          | 4.87     |\n",
      "| avg_norm_k_dot_g    | 4.55e+03 |\n",
      "| entropy             | 185      |\n",
      "| explained_variance  | 0.625    |\n",
      "| fps                 | 3098     |\n",
      "| loss                | 4.17e+03 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -9.85    |\n",
      "| loss_policy         | -9.85    |\n",
      "| loss_q              | 8.35e+03 |\n",
      "| mean_episode_length | 447      |\n",
      "| mean_episode_reward | 7.41e+03 |\n",
      "| norm_grads          | 2.66e+03 |\n",
      "| norm_grads_policy   | 199      |\n",
      "| norm_grads_q        | 2.65e+03 |\n",
      "| total_timesteps     | 8640480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 5.27     |\n",
      "| avg_norm_g          | 4.14e+03 |\n",
      "| avg_norm_grads_f    | 4.14e+03 |\n",
      "| avg_norm_k          | 5.37     |\n",
      "| avg_norm_k_dot_g    | 4.16e+03 |\n",
      "| entropy             | 230      |\n",
      "| explained_variance  | 0.259    |\n",
      "| fps                 | 3103     |\n",
      "| loss                | 2.34e+04 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 38.1     |\n",
      "| loss_policy         | 38.1     |\n",
      "| loss_q              | 4.68e+04 |\n",
      "| mean_episode_length | 456      |\n",
      "| mean_episode_reward | 7.65e+03 |\n",
      "| norm_grads          | 8.5e+03  |\n",
      "| norm_grads_policy   | 338      |\n",
      "| norm_grads_q        | 8.49e+03 |\n",
      "| total_timesteps     | 8880480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 21.3     |\n",
      "| avg_norm_g          | 1.79e+03 |\n",
      "| avg_norm_grads_f    | 1.78e+03 |\n",
      "| avg_norm_k          | 5.15     |\n",
      "| avg_norm_k_dot_g    | 1.6e+03  |\n",
      "| entropy             | 243      |\n",
      "| explained_variance  | 0.397    |\n",
      "| fps                 | 3108     |\n",
      "| loss                | 1.28e+04 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -5.8     |\n",
      "| loss_policy         | -5.8     |\n",
      "| loss_q              | 2.55e+04 |\n",
      "| mean_episode_length | 477      |\n",
      "| mean_episode_reward | 8.3e+03  |\n",
      "| norm_grads          | 1e+04    |\n",
      "| norm_grads_policy   | 230      |\n",
      "| norm_grads_q        | 1e+04    |\n",
      "| total_timesteps     | 9120480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 40.2     |\n",
      "| avg_norm_g          | 2.21e+03 |\n",
      "| avg_norm_grads_f    | 2.19e+03 |\n",
      "| avg_norm_k          | 5.24     |\n",
      "| avg_norm_k_dot_g    | 2.4e+03  |\n",
      "| entropy             | 223      |\n",
      "| explained_variance  | 0.407    |\n",
      "| fps                 | 3113     |\n",
      "| loss                | 3.16e+04 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -39.9    |\n",
      "| loss_policy         | -39.9    |\n",
      "| loss_q              | 6.34e+04 |\n",
      "| mean_episode_length | 486      |\n",
      "| mean_episode_reward | 8.5e+03  |\n",
      "| norm_grads          | 2.72e+04 |\n",
      "| norm_grads_policy   | 192      |\n",
      "| norm_grads_q        | 2.72e+04 |\n",
      "| total_timesteps     | 9360480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 4.86     |\n",
      "| avg_norm_g          | 5.27e+03 |\n",
      "| avg_norm_grads_f    | 5.26e+03 |\n",
      "| avg_norm_k          | 4.79     |\n",
      "| avg_norm_k_dot_g    | 5.1e+03  |\n",
      "| entropy             | 198      |\n",
      "| explained_variance  | 0.341    |\n",
      "| fps                 | 3118     |\n",
      "| loss                | 1.25e+04 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 16.6     |\n",
      "| loss_policy         | 16.6     |\n",
      "| loss_q              | 2.5e+04  |\n",
      "| mean_episode_length | 448      |\n",
      "| mean_episode_reward | 7.62e+03 |\n",
      "| norm_grads          | 4.19e+03 |\n",
      "| norm_grads_policy   | 187      |\n",
      "| norm_grads_q        | 4.18e+03 |\n",
      "| total_timesteps     | 9600480  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 11.5     |\n",
      "| avg_norm_g          | 1.32e+03 |\n",
      "| avg_norm_grads_f    | 1.32e+03 |\n",
      "| avg_norm_k          | 5.15     |\n",
      "| avg_norm_k_dot_g    | 1.31e+03 |\n",
      "| entropy             | 233      |\n",
      "| explained_variance  | 0.698    |\n",
      "| fps                 | 3123     |\n",
      "| loss                | 4.46e+03 |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -2.77    |\n",
      "| loss_policy         | -2.77    |\n",
      "| loss_q              | 8.93e+03 |\n",
      "| mean_episode_length | 422      |\n",
      "| mean_episode_reward | 7.1e+03  |\n",
      "| norm_grads          | 4.54e+03 |\n",
      "| norm_grads_policy   | 64.3     |\n",
      "| norm_grads_q        | 4.54e+03 |\n",
      "| total_timesteps     | 9840480  |\n",
      "----------------------------------\n",
      "--- Time take to train model = 53.0 minutes ---\n",
      "Saving Final Model...\n",
      "Model saved as ./ACER-nenv=24-tts=1e+07\n"
     ]
    }
   ],
   "source": [
    "# Refer to the stable baseline documentation for alternative implementations\n",
    "# of callbacks, baselines and others\n",
    "from stable_baselines.common.callbacks import CheckpointCallback\n",
    "from stable_baselines.common import make_vec_env\n",
    "from stable_baselines import ACER\n",
    "\n",
    "# fixed params for challenge\n",
    "# Total timesteps for training\n",
    "tts = int(1e7) \n",
    "\n",
    "# custom params\n",
    "n_envs = 24\n",
    "save_freq = int(1e5)\n",
    "log_interval = 500\n",
    "\n",
    "# Vectorize environment\n",
    "env = make_vec_env(env_id=\"diner_dash:DinerDash-v0\", n_envs=n_envs, wrapper_class=OneHotWrapper)\n",
    "\n",
    "# Initialise model\n",
    "model = ACER('MlpPolicy', env, verbose=1)\n",
    "\n",
    "# Initialise callback\n",
    "acer_callback = CheckpointCallback(save_freq=save_freq, save_path=f'./logs-ACER-nenv={n_envs}-tts={tts:.0e}/', name_prefix='diner-dash-PPO')\n",
    "\n",
    "# Train model\n",
    "start_time = time.time()\n",
    "model.learn(total_timesteps=tts, log_interval=log_interval, callback=acer_callback)\n",
    "print(f\"--- Time take to train model = {(time.time() - start_time)//60} minutes ---\")\n",
    "\n",
    "# Save model\n",
    "print(\"Saving Final Model...\")\n",
    "modelDirectory = \"./\"\n",
    "modelName = f\"ACER-nenv={n_envs}-tts={tts:.0e}\"\n",
    "model.save(modelDirectory + modelName)\n",
    "print(f\"Model saved as {modelDirectory + modelName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bh6oP_ofFjV_"
   },
   "outputs": [],
   "source": [
    "def testACER(seed):\n",
    "  from stable_baselines.common import make_vec_env\n",
    "  from stable_baselines import ACER\n",
    "\n",
    "  # Vectorize environment with given seed\n",
    "  env = make_vec_env(env_id=\"diner_dash:DinerDash-v0\", wrapper_class=OneHotWrapper, seed=seed)\n",
    "\n",
    "  # Load saved model\n",
    "  ACER_model = ACER.load(\"ACER-nenv=24-tts=1e+07\")\n",
    "\n",
    "  # Reset environment, init obs\n",
    "  obs = env.reset()\n",
    "\n",
    "  done = False\n",
    "  sum_rewards = 0\n",
    "  action_list = []\n",
    "\n",
    "  while not done:\n",
    "    action, _states = ACER_model.predict(obs)\n",
    "    action_list.append(action.item())\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    sum_rewards += rewards\n",
    "\n",
    "  return sum_rewards, action_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SolGR2yNPGCQ"
   },
   "source": [
    "# Testing of Policies and Verification of Submission [Please Edit]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x6rxjZmnBceL"
   },
   "source": [
    "Creates a json file of action lists from the best scoring algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JWh2kSf9OGv1"
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import json\n",
    "from os import getcwd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Sample test\n",
    "def test():\n",
    "  ############################ CHANGEABLE AREA ##############################\n",
    "  # Changeable parameters\n",
    "  numEpisodes = 100                             # num of test episodes\n",
    "  # algos = [testRA, testPPO, testPPO2]           # Add or remove algos (must have unique names)\n",
    "  algos = [testACER]           # Add or remove algos (must have unique names)\n",
    "  saveJson = True                              # Whether to save actions_dict\n",
    "  fileDirectory = \"./\"                          # Path of saved json file\n",
    "  fileName = \"submission.json\"                  # Name of json file\n",
    "\n",
    "  ### Replace the list of randomSeeds with that given for submission\n",
    "  # e.g. randomSeeds = [1, 2, 3]\n",
    "  # randomSeeds = [randint(0, 1e8) for i in range(numEpisodes)]\n",
    "  randomSeeds = [45990181, 42859851, 88292417, 17986451, 4310124, 28416871, 21378509, 28987250, 49793653, 81705172, 90381554, 13393105, 90402290, 69802779, 87378977, 7338848, 74942140, 86896376, 60192513, 90268611, 12193092, 45037492, 32444344, 60276470, 81720257, 48114169, 2745186, 39780027, 68039546, 63661496, 89673369, 54490252, 9508183, 78690722, 41872036, 40729179, 71091571, 52945376, 49602567, 11079941, 35506423, 32863705, 98722501, 95078645, 2050683, 30225876, 12983163, 5244339, 28278496, 80180211, 63902897, 46843366, 74357835, 90376940, 98407071, 48007796, 96438018, 54730109, 40955186, 60494091, 76878283, 24175421, 91447265, 36570693, 3334869, 14057265, 53946219, 30908957, 86325356, 90558192, 24759335, 51591742, 38364662, 1189567, 536631, 16559969, 68687507, 24406829, 9720389, 23088515, 34242387, 74268255, 23615670, 68613237, 7166219, 27203162, 29343492, 75431707, 39683866, 87146964, 78351462, 23184439, 9088138, 34637812, 25889305, 95479264, 55637910, 26835621, 37209126, 47123382]\n",
    "\n",
    "  ############################################################################\n",
    "\n",
    "  # uses function name as key\n",
    "  # hence, name function with algo name (e.g. testPPO or just PPO)\n",
    "  rewards_dict = {algo.__name__ : [] for algo in algos}\n",
    "  actions_dict = {algo.__name__ : [] for algo in algos}\n",
    "\n",
    "  # Test begins\n",
    "  for seed in tqdm(randomSeeds):\n",
    "    for algo in algos:\n",
    "      # Given a random seed\n",
    "      # Returns the sum of rewards for that episode and the actions list\n",
    "      rewards, actions = algo(seed)\n",
    "\n",
    "      rewards_dict[algo.__name__].append(rewards)\n",
    "      actions_dict[algo.__name__].append(actions)\n",
    "\n",
    "  # Print average rewards from n episodes for each algo\n",
    "  avgReward_dict = {algo : int(sum(rewards)/len(rewards)) for algo, rewards in rewards_dict.items()}\n",
    "  print(f\"Average Rewards for each algo: {avgReward_dict}\")\n",
    "\n",
    "  # Prints best algo\n",
    "  best_algo = max(avgReward_dict.keys(), key=(lambda k: avgReward_dict[k]))\n",
    "  best_reward = avgReward_dict[best_algo]\n",
    "  print(f\"The best algo is {best_algo} with the highest rewards of {best_reward}\")\n",
    "\n",
    "  # Print an action dict containing actions list for each random seed env for each algo\n",
    "  print(f\"Actions list for each env for each algo: {actions_dict}\")\n",
    "\n",
    "  submission_dict = {best_algo: actions_dict[best_algo]}\n",
    "\n",
    "  if saveJson:\n",
    "    print(\"Saving best algo to json file...\")\n",
    "    with open(fileDirectory + fileName, \"w\") as write_file:\n",
    "      json.dump(submission_dict, write_file)\n",
    "      print(f\"{fileName} was saved in {getcwd()}\")\n",
    "    \n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    print(f\"Verifying {fileName}...\")\n",
    "    (best_algo, best_action_list), = submission_dict.items()\n",
    "    print(f\"Name of best algo: {best_algo}\")\n",
    "    submissionEpisodes = len(best_action_list)\n",
    "    if submissionEpisodes != len(randomSeeds):\n",
    "      raise ValueError(\"Number of episodes in submission does not match the number of random seeds!\")\n",
    "    print(f\"Number of episodes(random seeds): {submissionEpisodes}\")\n",
    "    print(\"Number of episodes in submission matches the number of random seeds\")\n",
    "    print(\"Verification Complete! Please double check the verification results\")\n",
    "  \n",
    "  return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389,
     "referenced_widgets": [
      "b0c0435e3c0a46c4b3b607445381e6f2",
      "3d42387887584b67be7398716cdc63b4",
      "1d4bcc1454de42189a66a5b1b6ec87d8",
      "810783ac4a0444879f4043416c95f410",
      "b902996154624782a7cc23c4d1c0b070",
      "54e600315dbc448c8d489f21b751ef75",
      "96cac2d4f60b4faca5f38915a79309d2",
      "32053e3f09044c20b2842779bfd702a5"
     ]
    },
    "colab_type": "code",
    "id": "S9BwcjYfmyfI",
    "outputId": "9ba97e4a-e299-4feb-a2f1-f01b94e0823b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c0435e3c0a46c4b3b607445381e6f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-fbd55f77ab7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-64-0febdd71558c>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0;31m# Given a random seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0;31m# Returns the sum of rewards for that episode and the actions list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m       \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0mrewards_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-16d8381ecbe6>\u001b[0m in \u001b[0;36mtestACER\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m# Load saved model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mACER_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mACER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ACER-nenv=24-tts=1e+07\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;31m# Reset environment, init obs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stable_baselines/common/base_class.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, load_path, env, custom_objects, **kwargs)\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stable_baselines/acer/acer_simple.py\u001b[0m in \u001b[0;36mset_env\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0menv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_envs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                 \u001b[0;34m\"Error: the environment passed must have the same number of environments as the model was trained on.\"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m                 \u001b[0;34m\"This is due to ACER not being capable of changing the number of environments.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Error: the environment passed must have the same number of environments as the model was trained on.This is due to ACER not being capable of changing the number of environments."
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4aUwt6W97XL3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Challenge_alwy_01.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1d4bcc1454de42189a66a5b1b6ec87d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "  0%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54e600315dbc448c8d489f21b751ef75",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b902996154624782a7cc23c4d1c0b070",
      "value": 0
     }
    },
    "32053e3f09044c20b2842779bfd702a5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d42387887584b67be7398716cdc63b4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54e600315dbc448c8d489f21b751ef75": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "810783ac4a0444879f4043416c95f410": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_32053e3f09044c20b2842779bfd702a5",
      "placeholder": "​",
      "style": "IPY_MODEL_96cac2d4f60b4faca5f38915a79309d2",
      "value": " 0/100 [00:00&lt;?, ?it/s]"
     }
    },
    "96cac2d4f60b4faca5f38915a79309d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b0c0435e3c0a46c4b3b607445381e6f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1d4bcc1454de42189a66a5b1b6ec87d8",
       "IPY_MODEL_810783ac4a0444879f4043416c95f410"
      ],
      "layout": "IPY_MODEL_3d42387887584b67be7398716cdc63b4"
     }
    },
    "b902996154624782a7cc23c4d1c0b070": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
